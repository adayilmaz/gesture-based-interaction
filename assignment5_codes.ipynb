{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1hJcrtQbYOs"
   },
   "source": [
    "# HAND GESTURE-BASED INTERACTION\n",
    "\n",
    "---\n",
    "\n",
    "Group members:\n",
    "*   Ada YÄ±lmaz\n",
    "*   Ceren Åžahin\n",
    "*   Sima Adleyba\n",
    "*   List item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAjfOu8vbX9T"
   },
   "source": [
    "### Installing necessary libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUn2JcSgZ7Q7",
    "outputId": "9c342d67-09a3-467c-8af2-efcfc6585f03",
    "ExecuteTime": {
     "end_time": "2024-12-07T11:35:50.901108Z",
     "start_time": "2024-12-07T11:35:50.770710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "source": [
    "#install mediapipe\n",
    "!pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KtyLGHBsaCoN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#download a model that can recognize 7 hand gestures: ðŸ‘, ðŸ‘Ž, âœŒï¸, â˜ï¸, âœŠ, ðŸ‘‹, ðŸ¤Ÿ\n",
    "!wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j03y4pVScZr5",
    "ExecuteTime": {
     "end_time": "2024-12-07T12:14:44.482226Z",
     "start_time": "2024-12-07T12:14:42.409124Z"
    }
   },
   "outputs": [],
   "source": [
    "#download test images from pixabay\n",
    "import urllib\n",
    "\n",
    "IMAGE_FILENAMES = ['thumbs_down.jpg', 'victory.jpg', 'thumbs_up.jpg', 'pointing_up.jpg']\n",
    "\n",
    "for name in IMAGE_FILENAMES:\n",
    "  url = f'https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/{name}'\n",
    "  urllib.request.urlretrieve(url, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a_QNcKXCdAuq",
    "ExecuteTime": {
     "end_time": "2024-12-07T12:14:55.808328Z",
     "start_time": "2024-12-07T12:14:55.781972Z"
    }
   },
   "outputs": [],
   "source": [
    "#or we can use our own images as shown below\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for filename in uploaded:\n",
    "#   content = uploaded[filename]\n",
    "#   with open(filename, 'wb') as f:\n",
    "#     f.write(content)\n",
    "# IMAGE_FILENAMES = list(uploaded.keys())\n",
    "\n",
    "# print('Uploaded files:', IMAGE_FILENAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WqLvhGEczPc"
   },
   "source": [
    "### Functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eZQdCS6uaRny",
    "ExecuteTime": {
     "end_time": "2024-12-07T12:14:58.619392Z",
     "start_time": "2024-12-07T12:14:56.913053Z"
    }
   },
   "outputs": [],
   "source": [
    "#some functions to visualize the gesture recognition results.\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'xtick.labelbottom': False,\n",
    "    'xtick.bottom': False,\n",
    "    'ytick.labelleft': False,\n",
    "    'ytick.left': False,\n",
    "    'xtick.labeltop': False,\n",
    "    'xtick.top': False,\n",
    "    'ytick.labelright': False,\n",
    "    'ytick.right': False\n",
    "})\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "\n",
    "def display_one_image(image, title, subplot, titlesize=16):\n",
    "    \"\"\"Displays one image along with the predicted category name and score.\"\"\"\n",
    "    plt.subplot(*subplot)\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "\n",
    "\n",
    "def display_batch_of_images_with_gestures_and_hand_landmarks(images, results):\n",
    "    \"\"\"Displays a batch of images with the gesture category and its score along with the hand landmarks.\"\"\"\n",
    "    # Images and labels.\n",
    "    images = [image.numpy_view() for image in images]\n",
    "    gestures = [top_gesture for (top_gesture, _) in results]\n",
    "    multi_hand_landmarks_list = [multi_hand_landmarks for (_, multi_hand_landmarks) in results]\n",
    "\n",
    "    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images) // rows\n",
    "\n",
    "    # Size and spacing.\n",
    "    FIGSIZE = 13.0\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols, 1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "\n",
    "    # Display gestures and hand landmarks.\n",
    "    for i, (image, gestures) in enumerate(zip(images[:rows*cols], gestures[:rows*cols])):\n",
    "        title = f\"{gestures.category_name} ({gestures.score:.2f})\"\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3\n",
    "        annotated_image = image.copy()\n",
    "\n",
    "        for hand_landmarks in multi_hand_landmarks_list[i]:\n",
    "          hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "          hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "          ])\n",
    "\n",
    "          mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        subplot = display_one_image(annotated_image, title, subplot, titlesize=dynamic_titlesize)\n",
    "\n",
    "    # Layout.\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnVjzQPedaun"
   },
   "source": [
    "### Preview the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m9uzHARZddDK",
    "outputId": "42c11584-885e-4a08-ee4c-a81189d64573",
    "ExecuteTime": {
     "end_time": "2024-12-07T12:15:08.042600Z",
     "start_time": "2024-12-07T12:15:04.469527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: thumbs_down.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 15:15:04.982 Python[11988:558908] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-12-07 15:15:04.982 Python[11988:558908] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: victory.jpg\n",
      "Displaying: thumbs_up.jpg\n",
      "Displaying: pointing_up.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "\n",
    "DESIRED_HEIGHT = 480\n",
    "DESIRED_WIDTH = 480\n",
    "\n",
    "def resize_and_show(image):\n",
    "    h, w = image.shape[:2]\n",
    "    if h < w:\n",
    "        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h / (w / DESIRED_WIDTH))))\n",
    "    else:\n",
    "        img = cv2.resize(image, (math.floor(w / (h / DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
    "    \n",
    "    # Display the image in a window with a name\n",
    "    cv2.imshow('Resized Image', img)\n",
    "    cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "    cv2.destroyAllWindows()  # Close the window after key press\n",
    "\n",
    "# Example usage\n",
    "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
    "\n",
    "for name, image in images.items():\n",
    "    if image is not None:\n",
    "        print(f\"Displaying: {name}\")\n",
    "        resize_and_show(image)\n",
    "    else:\n",
    "        print(f\"Error: Could not read image {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zRNOZm7d5Wr"
   },
   "source": [
    "### Run the Google implementation\n",
    "This first one is a how to from google, we can create our own by following the steps given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JwAdQ_VjeAku",
    "outputId": "f8982a23-7eff-456b-b915-af0daf1ec27f",
    "ExecuteTime": {
     "end_time": "2024-12-07T12:15:15.574996Z",
     "start_time": "2024-12-07T12:15:15.189956Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733573715.162574  558908 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1733573715.162988  558908 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1733573715.163858  558908 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733573715.174755  560017 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733573715.183577  560017 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733573715.184255  560016 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733573715.184308  560016 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733573715.234642  560018 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input image must contain three channel bgr data.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 26\u001B[0m\n\u001B[1;32m     23\u001B[0m   hand_landmarks \u001B[38;5;241m=\u001B[39m recognition_result\u001B[38;5;241m.\u001B[39mhand_landmarks\n\u001B[1;32m     24\u001B[0m   results\u001B[38;5;241m.\u001B[39mappend((top_gesture, hand_landmarks))\n\u001B[0;32m---> 26\u001B[0m \u001B[43mdisplay_batch_of_images_with_gestures_and_hand_landmarks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresults\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[3], line 68\u001B[0m, in \u001B[0;36mdisplay_batch_of_images_with_gestures_and_hand_landmarks\u001B[0;34m(images, results)\u001B[0m\n\u001B[1;32m     63\u001B[0m       hand_landmarks_proto \u001B[38;5;241m=\u001B[39m landmark_pb2\u001B[38;5;241m.\u001B[39mNormalizedLandmarkList()\n\u001B[1;32m     64\u001B[0m       hand_landmarks_proto\u001B[38;5;241m.\u001B[39mlandmark\u001B[38;5;241m.\u001B[39mextend([\n\u001B[1;32m     65\u001B[0m         landmark_pb2\u001B[38;5;241m.\u001B[39mNormalizedLandmark(x\u001B[38;5;241m=\u001B[39mlandmark\u001B[38;5;241m.\u001B[39mx, y\u001B[38;5;241m=\u001B[39mlandmark\u001B[38;5;241m.\u001B[39my, z\u001B[38;5;241m=\u001B[39mlandmark\u001B[38;5;241m.\u001B[39mz) \u001B[38;5;28;01mfor\u001B[39;00m landmark \u001B[38;5;129;01min\u001B[39;00m hand_landmarks\n\u001B[1;32m     66\u001B[0m       ])\n\u001B[0;32m---> 68\u001B[0m       \u001B[43mmp_drawing\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw_landmarks\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m        \u001B[49m\u001B[43mannotated_image\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhand_landmarks_proto\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmp_hands\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHAND_CONNECTIONS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmp_drawing_styles\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_default_hand_landmarks_style\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmp_drawing_styles\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_default_hand_connections_style\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m     subplot \u001B[38;5;241m=\u001B[39m display_one_image(annotated_image, title, subplot, titlesize\u001B[38;5;241m=\u001B[39mdynamic_titlesize)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m# Layout.\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/mediapipe/python/solutions/drawing_utils.py:158\u001B[0m, in \u001B[0;36mdraw_landmarks\u001B[0;34m(image, landmark_list, connections, landmark_drawing_spec, connection_drawing_spec, is_drawing_landmarks)\u001B[0m\n\u001B[1;32m    156\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m image\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m!=\u001B[39m _BGR_CHANNELS:\n\u001B[0;32m--> 158\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput image must contain three channel bgr data.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    159\u001B[0m image_rows, image_cols, _ \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    160\u001B[0m idx_to_coordinates \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[0;31mValueError\u001B[0m: Input image must contain three channel bgr data."
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1300x1300 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an GestureRecognizer object.\n",
    "base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')\n",
    "options = vision.GestureRecognizerOptions(base_options=base_options)\n",
    "recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "\n",
    "images = []\n",
    "results = []\n",
    "for image_file_name in IMAGE_FILENAMES:\n",
    "    \n",
    "  # STEP 3: Load the input image.\n",
    "  image = mp.Image.create_from_file(image_file_name)\n",
    "\n",
    "  # STEP 4: Recognize gestures in the input image.\n",
    "  recognition_result = recognizer.recognize(image)\n",
    "\n",
    "  # STEP 5: Process the result. In this case, visualize it.\n",
    "  images.append(image)\n",
    "  top_gesture = recognition_result.gestures[0][0]\n",
    "  hand_landmarks = recognition_result.hand_landmarks\n",
    "  results.append((top_gesture, hand_landmarks))\n",
    "\n",
    "display_batch_of_images_with_gestures_and_hand_landmarks(images, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example\n",
    "Tried an example to see how it works and update the functions accordingly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vxV_DPPWeBOo",
    "ExecuteTime": {
     "end_time": "2024-12-07T12:11:34.835600Z",
     "start_time": "2024-12-07T12:11:33.832301Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def detect_peace_sign(hand_landmarks):\n",
    "    # Get coordinates of relevant landmarks\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "    \n",
    "    \n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    middle_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    pinky_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP]\n",
    "\n",
    "\n",
    "    # Define peace sign gesture: index and middle fingers up, ring and pinky fingers down\n",
    "    if (\n",
    "        # Index and middle fingers' tips must be highest parts of those fingers\n",
    "        (index_tip.y < index_mcp.y) and (middle_tip.y < middle_mcp.y)\n",
    "        and\n",
    "        # Index finger's tips must be higher than other fingers' mcp\n",
    "        (index_tip.y < ring_mcp.y) and (index_tip.y < pinky_mcp.y)\n",
    "        and\n",
    "        # Middle finger's tips must be higher than other fingers' mcp\n",
    "        (middle_tip.y < ring_mcp.y) and (middle_tip.y < pinky_mcp.y)\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Detect custom gesture (peace sign)\n",
    "            if detect_peace_sign(hand_landmarks):\n",
    "                cv2.putText(frame, 'Peace Sign Detected!', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                # Trigger interaction (e.g., print a message)\n",
    "                print(\"Peace sign gesture detected!\")\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('MediaPipe Hands', frame)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gesture Detection Functions\n",
    "After checking how the upper ones worked, we implemented fixed, more robust versions of gesture detection functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Global variables for tracking gestures and cooldown\n",
    "\n",
    "# The currently detected gesture\n",
    "current_gesture = None\n",
    "\n",
    "# Time when the current gesture expires\n",
    "gesture_reset_time = 0\n",
    "\n",
    "# Cooldown variables for scrolling\n",
    "previous_thumb_tip = None\n",
    "previous_index_tip = None\n",
    "last_gesture_time = 0\n",
    "COOLDOWN_PERIOD = 1.5\n",
    "\n",
    "# Reset current gesture when it expires\n",
    "def reset_gesture():\n",
    "    global current_gesture, gesture_reset_time\n",
    "    if time.time() > gesture_reset_time:\n",
    "        current_gesture = None\n",
    "\n",
    "# Gesture detection functions\n",
    "def detect_peace_sign(hand_landmarks):\n",
    "    \n",
    "    # Track the time of the last detected gesture\n",
    "    global last_gesture_time\n",
    "\n",
    "    # Get landmarks\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    middle_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    pinky_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP]\n",
    "\n",
    "    # Check that index and middle are raised above their MCPs and other MCPs\n",
    "    index_and_middle_up = (\n",
    "        (index_tip.y < index_mcp.y) and\n",
    "        (middle_tip.y < middle_mcp.y) and\n",
    "        (index_tip.y < ring_mcp.y) and \n",
    "        (index_tip.y < pinky_mcp.y) and\n",
    "        (middle_tip.y < ring_mcp.y) and \n",
    "        (middle_tip.y < pinky_mcp.y)\n",
    "    )\n",
    "\n",
    "    # Check spacing between index and middle fingers\n",
    "    index_middle_spacing = abs(index_tip.x - middle_tip.x) > 0.1\n",
    "\n",
    "    # Check that ring and pinky are down (their tips should be below their MCP joints)\n",
    "    ring_and_pinky_down = (\n",
    "        (ring_tip.y > ring_mcp.y + 0.02) and\n",
    "        (pinky_tip.y > pinky_mcp.y + 0.02)\n",
    "    )\n",
    "\n",
    "    if index_and_middle_up and index_middle_spacing and ring_and_pinky_down:\n",
    "        \n",
    "        # Update last gesture time (to apply cooldown for peace sign gesture)\n",
    "        last_gesture_time = time.time()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def detect_thumbs_up(hand_landmarks, margin=0.05):\n",
    "    \n",
    "    # Track the time of the last detected gesture\n",
    "    global last_gesture_time \n",
    "\n",
    "    # Get landmarks\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    middle_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    pinky_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP]\n",
    "    thumb_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP]\n",
    "    \n",
    "    thumb_base = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_CMC]\n",
    "\n",
    "    # Thumb tip should be above other fingertips\n",
    "    thumb_tip_up = ((thumb_tip.y + margin < index_tip.y) and\n",
    "                    (thumb_tip.y + margin < middle_tip.y) and\n",
    "                    (thumb_tip.y + margin < ring_tip.y) and\n",
    "                    (thumb_tip.y + margin < pinky_tip.y) and\n",
    "                    (thumb_tip.y < thumb_mcp.y))\n",
    "    \n",
    "    # Other fingers should be in order from top to bottom\n",
    "    other_fingers_ordered = ((index_mcp.y < middle_mcp.y) and\n",
    "                             (middle_mcp.y < ring_mcp.y) and\n",
    "                             (ring_mcp.y < pinky_mcp.y))\n",
    "    \n",
    "    \n",
    "    if thumb_tip_up and other_fingers_ordered:\n",
    "        \n",
    "        # Update last gesture time (to apply cooldown for thumbs-up gesture)\n",
    "        last_gesture_time = time.time()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_thumbs_down(hand_landmarks, margin=0.05):\n",
    "    \n",
    "    # Track the time of the last detected gesture\n",
    "    global last_gesture_time\n",
    "\n",
    "    # Get landmarks\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    middle_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    pinky_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP]\n",
    "\n",
    "    # Thumb tip must be lower than anything else\n",
    "    thumb_tip_down = ((thumb_tip.y > index_tip.y + margin) and\n",
    "                    (thumb_tip.y > middle_tip.y + margin) and\n",
    "                    (thumb_tip.y > ring_tip.y + margin) and\n",
    "                    (thumb_tip.y > pinky_tip.y + margin))\n",
    "    \n",
    "    # Other fingers should be in the order (from top to down) pinky > ring > middle > index finger\n",
    "    other_fingers_ordered = ((index_mcp.y > middle_mcp.y) and\n",
    "                             (middle_mcp.y > ring_mcp.y) and\n",
    "                             (ring_mcp.y > pinky_mcp.y))\n",
    "    \n",
    "    if thumb_tip_down and other_fingers_ordered:\n",
    "        \n",
    "        # Update last gesture time (to apply cooldown for thumbs-down gesture)\n",
    "        last_gesture_time = time.time()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detect_scroll(hand_landmarks, threshold=0.1, dominance_ratio=4.0):\n",
    "    \n",
    "    # Get previous positions and last gesture time\n",
    "    global previous_thumb_tip, previous_index_tip, last_gesture_time\n",
    "\n",
    "    # Get current positions\n",
    "    current_thumb_tip_x = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].x\n",
    "    current_thumb_tip_y = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y\n",
    "    current_index_tip_x = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x\n",
    "    current_index_tip_y = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y\n",
    "\n",
    "    # Check for cooldown\n",
    "    current_time = time.time()\n",
    "    \n",
    "    # If last gesture happened not before cooldown, return False (no scroll)\n",
    "    if current_time - last_gesture_time < COOLDOWN_PERIOD:\n",
    "        return False, None \n",
    "\n",
    "    # Initialize previous positions if not set\n",
    "    if previous_thumb_tip is None or previous_index_tip is None:\n",
    "        previous_thumb_tip = (current_thumb_tip_x, current_thumb_tip_y)\n",
    "        previous_index_tip = (current_index_tip_x, current_index_tip_y)\n",
    "        return False, None\n",
    "\n",
    "    # Calculate location changes\n",
    "    thumb_horizontal_disp = current_thumb_tip_x - previous_thumb_tip[0]\n",
    "    thumb_vertical_disp = current_thumb_tip_y - previous_thumb_tip[1]\n",
    "    index_horizontal_disp = current_index_tip_x - previous_index_tip[0]\n",
    "    index_vertical_disp = current_index_tip_y - previous_index_tip[1]\n",
    "\n",
    "    # Average the movements of thumb and index for robustness\n",
    "    horizontal_disp = (thumb_horizontal_disp + index_horizontal_disp) / 2\n",
    "    vertical_disp = (thumb_vertical_disp + index_vertical_disp) / 2\n",
    "\n",
    "    # Determine dominant movement (we want to return only a horizontal or vertical movement)\n",
    "    horizontal_movement = abs(horizontal_disp) > threshold\n",
    "    vertical_movement = abs(vertical_disp) > threshold\n",
    "\n",
    "    # Check dominance\n",
    "    if horizontal_movement and abs(horizontal_disp) > dominance_ratio * abs(vertical_disp):\n",
    "        direction = \"right\" if horizontal_disp > 0 else \"left\"\n",
    "        dominant_axis = \"horizontal\"\n",
    "    elif vertical_movement and abs(vertical_disp) > dominance_ratio * abs(horizontal_disp):\n",
    "        direction = \"down\" if vertical_disp > 0 else \"up\"\n",
    "        dominant_axis = \"vertical\"\n",
    "    else:\n",
    "        direction = None\n",
    "        dominant_axis = None\n",
    "\n",
    "    # If there is a dominant movement\n",
    "    if dominant_axis:\n",
    "        \n",
    "        # Update positions\n",
    "        previous_thumb_tip = (current_thumb_tip_x, current_thumb_tip_y)\n",
    "        previous_index_tip = (current_index_tip_x, current_index_tip_y)\n",
    "        \n",
    "        # Update last gesture time\n",
    "        last_gesture_time = current_time \n",
    "        return True, direction\n",
    "\n",
    "    return False, None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-07T16:18:18.315785Z",
     "start_time": "2024-12-07T16:18:18.309652Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T16:18:41.742014Z",
     "start_time": "2024-12-07T16:18:19.305338Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733588299.302138  653089 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1733588299.310918  697430 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733588299.317616  697432 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'Esc' to exit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Global variables for tracking previous positions\n",
    "previous_thumb_tip = None\n",
    "previous_index_tip = None\n",
    "\n",
    "print(\"Press 'Esc' to exit.\")\n",
    "\n",
    "# Initialize gesture display variables\n",
    "current_gesture = None \n",
    "gesture_display_time = 0 \n",
    "GESTURE_DISPLAY_DURATION = 1.5 \n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Detect gestures\n",
    "            detected_gesture = None\n",
    "            if detect_peace_sign(hand_landmarks):\n",
    "                detected_gesture = \"Peace Sign Detected!\"\n",
    "            elif detect_thumbs_up(hand_landmarks):\n",
    "                detected_gesture = \"Thumbs Up Detected!\"\n",
    "            elif detect_thumbs_down(hand_landmarks):\n",
    "                detected_gesture = \"Thumbs Down Detected!\"\n",
    "            \n",
    "            # If a gesture is detected, update the display variables\n",
    "            if detected_gesture:\n",
    "                current_gesture = detected_gesture\n",
    "                gesture_display_time = time.time()\n",
    "            else:\n",
    "                # Only check for scroll if no other gesture is detected\n",
    "                detected, direction = detect_scroll(hand_landmarks, threshold=0.1, dominance_ratio=4.0)\n",
    "                if detected:\n",
    "                    current_gesture = f\"Scroll Detected: {direction.title()}!\"\n",
    "                    gesture_display_time = time.time()\n",
    "\n",
    "    # Display the current gesture if within the display duration\n",
    "    if current_gesture and (time.time() - gesture_display_time < GESTURE_DISPLAY_DURATION):\n",
    "        cv2.putText(frame, current_gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Gesture Recognition', frame)\n",
    "\n",
    "    # Break the loop when 'Esc' key is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cursor control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T13:10:38.815812Z",
     "start_time": "2024-12-07T13:10:27.037631Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733577027.043797  567238 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "W0000 00:00:1733577027.055785  601304 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733577027.062216  601304 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'Esc' to exit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Get screen dimensions\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "def is_click_gesture(hand_landmarks):\n",
    "    \"\"\"Detect a pinching gesture for a left click.\"\"\"\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "    # Calculate the 3D Euclidean distance between index tip and thumb tip\n",
    "    distance = ((index_tip.x - thumb_tip.x) ** 2 +\n",
    "                (index_tip.y - thumb_tip.y) ** 2 +\n",
    "                (index_tip.z - thumb_tip.z) ** 2) ** 0.5\n",
    "\n",
    "    # Adjust threshold based on typical 3D distances observed\n",
    "    return distance < 0.05\n",
    "\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Press 'Esc' to exit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a mirror-like effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get index finger tip coordinates\n",
    "            index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "            # Normalize coordinates to screen dimensions\n",
    "            cursor_x = int(index_finger_tip.x * screen_width)\n",
    "            cursor_y = int(index_finger_tip.y * screen_height)\n",
    "\n",
    "            # Move the mouse cursor\n",
    "            pyautogui.moveTo(cursor_x, cursor_y)\n",
    "\n",
    "            # Detect click gesture\n",
    "            if is_click_gesture(hand_landmarks):\n",
    "                pyautogui.click()\n",
    "                cv2.putText(frame, \"Click!\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Gesture-Based Cursor Control\", frame)\n",
    "\n",
    "    # Break the loop when 'Esc' key is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RAjfOu8vbX9T",
    "5WqLvhGEczPc",
    "AnVjzQPedaun"
   ],
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
