{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1hJcrtQbYOs"
   },
   "source": [
    "# HAND GESTURE-BASED INTERACTION\n",
    "\n",
    "---\n",
    "\n",
    "Group members:\n",
    "*   Ada Yılmaz\n",
    "*   Ceren Şahin\n",
    "*   Sima Adleyba\n",
    "*   Selen Naz Gürsoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAjfOu8vbX9T"
   },
   "source": [
    "### Installing necessary libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T11:35:50.901108Z",
     "start_time": "2024-12-07T11:35:50.770710Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUn2JcSgZ7Q7",
    "outputId": "9c342d67-09a3-467c-8af2-efcfc6585f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install mediapipe\n",
    "%pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KtyLGHBsaCoN"
   },
   "outputs": [],
   "source": [
    "#download a model that can recognize 7 hand gestures: 👍, 👎, ✌️, ☝️, ✊, 👋, 🤟\n",
    "!wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T12:14:44.482226Z",
     "start_time": "2024-12-07T12:14:42.409124Z"
    },
    "id": "j03y4pVScZr5"
   },
   "outputs": [],
   "source": [
    "#download test images from pixabay\n",
    "import urllib\n",
    "\n",
    "IMAGE_FILENAMES = ['thumbs_down.jpg', 'victory.jpg', 'thumbs_up.jpg', 'pointing_up.jpg']\n",
    "\n",
    "for name in IMAGE_FILENAMES:\n",
    "  url = f'https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/{name}'\n",
    "  urllib.request.urlretrieve(url, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T12:14:55.808328Z",
     "start_time": "2024-12-07T12:14:55.781972Z"
    },
    "id": "a_QNcKXCdAuq"
   },
   "outputs": [],
   "source": [
    "#or we can use our own images as shown below\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for filename in uploaded:\n",
    "#   content = uploaded[filename]\n",
    "#   with open(filename, 'wb') as f:\n",
    "#     f.write(content)\n",
    "# IMAGE_FILENAMES = list(uploaded.keys())\n",
    "\n",
    "# print('Uploaded files:', IMAGE_FILENAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WqLvhGEczPc"
   },
   "source": [
    "### Functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T12:14:58.619392Z",
     "start_time": "2024-12-07T12:14:56.913053Z"
    },
    "id": "eZQdCS6uaRny"
   },
   "outputs": [],
   "source": [
    "#some functions to visualize the gesture recognition results.\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'xtick.labelbottom': False,\n",
    "    'xtick.bottom': False,\n",
    "    'ytick.labelleft': False,\n",
    "    'ytick.left': False,\n",
    "    'xtick.labeltop': False,\n",
    "    'xtick.top': False,\n",
    "    'ytick.labelright': False,\n",
    "    'ytick.right': False\n",
    "})\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "\n",
    "def display_one_image(image, title, subplot, titlesize=16):\n",
    "    \"\"\"Displays one image along with the predicted category name and score.\"\"\"\n",
    "    plt.subplot(*subplot)\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "\n",
    "\n",
    "def display_batch_of_images_with_gestures_and_hand_landmarks(images, results):\n",
    "    \"\"\"Displays a batch of images with the gesture category and its score along with the hand landmarks.\"\"\"\n",
    "    # Images and labels.\n",
    "    images = [image.numpy_view() for image in images]\n",
    "    gestures = [top_gesture for (top_gesture, _) in results]\n",
    "    multi_hand_landmarks_list = [multi_hand_landmarks for (_, multi_hand_landmarks) in results]\n",
    "\n",
    "    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images) // rows\n",
    "\n",
    "    # Size and spacing.\n",
    "    FIGSIZE = 13.0\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols, 1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "\n",
    "    # Display gestures and hand landmarks.\n",
    "    for i, (image, gestures) in enumerate(zip(images[:rows*cols], gestures[:rows*cols])):\n",
    "        title = f\"{gestures.category_name} ({gestures.score:.2f})\"\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3\n",
    "        annotated_image = image.copy()\n",
    "\n",
    "        for hand_landmarks in multi_hand_landmarks_list[i]:\n",
    "          hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "          hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "          ])\n",
    "\n",
    "          mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        subplot = display_one_image(annotated_image, title, subplot, titlesize=dynamic_titlesize)\n",
    "\n",
    "    # Layout.\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnVjzQPedaun"
   },
   "source": [
    "### Preview the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T12:15:08.042600Z",
     "start_time": "2024-12-07T12:15:04.469527Z"
    },
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m9uzHARZddDK",
    "outputId": "42c11584-885e-4a08-ee4c-a81189d64573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: thumbs_down.jpg\n",
      "Displaying: victory.jpg\n",
      "Displaying: thumbs_up.jpg\n",
      "Displaying: pointing_up.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "\n",
    "DESIRED_HEIGHT = 480\n",
    "DESIRED_WIDTH = 480\n",
    "\n",
    "def resize_and_show(image, name):\n",
    "    h, w = image.shape[:2]\n",
    "    if h < w:\n",
    "        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h / (w / DESIRED_WIDTH))))\n",
    "    else:\n",
    "        img = cv2.resize(image, (math.floor(w / (h / DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
    "    \n",
    "    # Display the image in a window with a name\n",
    "    cv2.imshow(name, img)\n",
    "    cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "    cv2.destroyAllWindows()  # Close the window after key press\n",
    "\n",
    "# Example usage\n",
    "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
    "\n",
    "for name, image in images.items():\n",
    "    if image is not None:\n",
    "        print(f\"Displaying: {name}\")\n",
    "        resize_and_show(image, name)\n",
    "    else:\n",
    "        print(f\"Error: Could not read image {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zRNOZm7d5Wr"
   },
   "source": [
    "### Google implementation\n",
    "This first one is a how to from google, we can create our own by following the steps given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T12:15:15.574996Z",
     "start_time": "2024-12-07T12:15:15.189956Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JwAdQ_VjeAku",
    "outputId": "f8982a23-7eff-456b-b915-af0daf1ec27f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733734701.863207   14037 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine\n",
      "W0000 00:00:1733734701.909597   14037 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1733734701.932447   14037 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733734702.018345   15212 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733734702.089356   15212 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733734702.094463   15215 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733734702.094618   15215 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733734702.162484   15211 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input image must contain three channel bgr data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m   hand_landmarks \u001b[38;5;241m=\u001b[39m recognition_result\u001b[38;5;241m.\u001b[39mhand_landmarks\n\u001b[1;32m     25\u001b[0m   results\u001b[38;5;241m.\u001b[39mappend((top_gesture, hand_landmarks))\n\u001b[0;32m---> 27\u001b[0m \u001b[43mdisplay_batch_of_images_with_gestures_and_hand_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 68\u001b[0m, in \u001b[0;36mdisplay_batch_of_images_with_gestures_and_hand_landmarks\u001b[0;34m(images, results)\u001b[0m\n\u001b[1;32m     63\u001b[0m       hand_landmarks_proto \u001b[38;5;241m=\u001b[39m landmark_pb2\u001b[38;5;241m.\u001b[39mNormalizedLandmarkList()\n\u001b[1;32m     64\u001b[0m       hand_landmarks_proto\u001b[38;5;241m.\u001b[39mlandmark\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m     65\u001b[0m         landmark_pb2\u001b[38;5;241m.\u001b[39mNormalizedLandmark(x\u001b[38;5;241m=\u001b[39mlandmark\u001b[38;5;241m.\u001b[39mx, y\u001b[38;5;241m=\u001b[39mlandmark\u001b[38;5;241m.\u001b[39my, z\u001b[38;5;241m=\u001b[39mlandmark\u001b[38;5;241m.\u001b[39mz) \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m hand_landmarks\n\u001b[1;32m     66\u001b[0m       ])\n\u001b[0;32m---> 68\u001b[0m       \u001b[43mmp_drawing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_landmarks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mannotated_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhand_landmarks_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmp_hands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHAND_CONNECTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmp_drawing_styles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_hand_landmarks_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmp_drawing_styles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_hand_connections_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     subplot \u001b[38;5;241m=\u001b[39m display_one_image(annotated_image, title, subplot, titlesize\u001b[38;5;241m=\u001b[39mdynamic_titlesize)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Layout.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/mediapipe/python/solutions/drawing_utils.py:158\u001b[0m, in \u001b[0;36mdraw_landmarks\u001b[0;34m(image, landmark_list, connections, landmark_drawing_spec, connection_drawing_spec, is_drawing_landmarks)\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m _BGR_CHANNELS:\n\u001b[0;32m--> 158\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel bgr data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    159\u001b[0m image_rows, image_cols, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    160\u001b[0m idx_to_coordinates \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Input image must contain three channel bgr data."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1300x1300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an GestureRecognizer object.\n",
    "base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')\n",
    "options = vision.GestureRecognizerOptions(base_options=base_options)\n",
    "recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "\n",
    "images = []\n",
    "results = []\n",
    "for image_file_name in IMAGE_FILENAMES:\n",
    "    \n",
    "  # STEP 3: Load the input image.\n",
    "  image = mp.Image.create_from_file(image_file_name)\n",
    "\n",
    "  # STEP 4: Recognize gestures in the input image.\n",
    "  recognition_result = recognizer.recognize(image)\n",
    "\n",
    "  # STEP 5: Process the result. In this case, visualize it.\n",
    "  images.append(image)\n",
    "  top_gesture = recognition_result.gestures[0][0]\n",
    "  hand_landmarks = recognition_result.hand_landmarks\n",
    "  results.append((top_gesture, hand_landmarks))\n",
    "\n",
    "display_batch_of_images_with_gestures_and_hand_landmarks(images, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Example\n",
    "Tried an example to see how it works and update the functions accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T12:11:34.835600Z",
     "start_time": "2024-12-07T12:11:33.832301Z"
    },
    "id": "vxV_DPPWeBOo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733734784.299569   14037 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine\n",
      "W0000 00:00:1733734784.420640   16680 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733734784.470914   16680 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-12-09 11:59:44.968 Python[1038:14037] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def detect_peace_sign(hand_landmarks):\n",
    "    # Get coordinates of relevant landmarks\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "    \n",
    "    \n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    middle_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    pinky_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP]\n",
    "\n",
    "\n",
    "    # Define peace sign gesture: index and middle fingers up, ring and pinky fingers down\n",
    "    if (\n",
    "        # Index and middle fingers' tips must be highest parts of those fingers\n",
    "        (index_tip.y < index_mcp.y) and (middle_tip.y < middle_mcp.y)\n",
    "        and\n",
    "        # Index finger's tips must be higher than other fingers' mcp\n",
    "        (index_tip.y < ring_mcp.y) and (index_tip.y < pinky_mcp.y)\n",
    "        and\n",
    "        # Middle finger's tips must be higher than other fingers' mcp\n",
    "        (middle_tip.y < ring_mcp.y) and (middle_tip.y < pinky_mcp.y)\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Detect custom gesture (peace sign)\n",
    "            if detect_peace_sign(hand_landmarks):\n",
    "                cv2.putText(frame, 'Peace Sign Detected!', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                # Trigger interaction (e.g., print a message)\n",
    "                print(\"Peace sign gesture detected!\")\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('MediaPipe Hands', frame)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Gesture Detection Functions\n",
    "After checking how the upper ones worked, we implemented fixed, more robust versions of gesture detection functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T16:18:18.315785Z",
     "start_time": "2024-12-07T16:18:18.309652Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Global variables for tracking gestures and cooldown\n",
    "\n",
    "# The currently detected gesture\n",
    "current_gesture = None\n",
    "\n",
    "# Time when the current gesture expires\n",
    "gesture_reset_time = 0\n",
    "\n",
    "# Cooldown variables for scrolling\n",
    "previous_thumb_tip = None\n",
    "previous_index_tip = None\n",
    "last_gesture_time = 0\n",
    "COOLDOWN_PERIOD = 1.5\n",
    "\n",
    "# Reset current gesture when it expires\n",
    "def reset_gesture():\n",
    "    global current_gesture, gesture_reset_time\n",
    "    if time.time() > gesture_reset_time:\n",
    "        current_gesture = None\n",
    "\n",
    "# Gesture detection functions\n",
    "def detect_peace_sign(hand_landmarks):\n",
    "    \n",
    "    # Track the time of the last detected gesture\n",
    "    global last_gesture_time\n",
    "\n",
    "    # Get landmarks\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    middle_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    pinky_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP]\n",
    "\n",
    "    # Check that index and middle are raised above their MCPs and other MCPs\n",
    "    index_and_middle_up = (\n",
    "        (index_tip.y < index_mcp.y) and\n",
    "        (middle_tip.y < middle_mcp.y) and\n",
    "        (index_tip.y < ring_mcp.y) and \n",
    "        (index_tip.y < pinky_mcp.y) and\n",
    "        (middle_tip.y < ring_mcp.y) and \n",
    "        (middle_tip.y < pinky_mcp.y)\n",
    "    )\n",
    "\n",
    "    # Check spacing between index and middle fingers\n",
    "    index_middle_spacing = abs(index_tip.x - middle_tip.x) > 0.1\n",
    "\n",
    "    # Check that ring and pinky are down (their tips should be below their MCP joints)\n",
    "    ring_and_pinky_down = (\n",
    "        (ring_tip.y > ring_mcp.y + 0.02) and\n",
    "        (pinky_tip.y > pinky_mcp.y + 0.02)\n",
    "    )\n",
    "\n",
    "    if index_and_middle_up and index_middle_spacing and ring_and_pinky_down:\n",
    "        \n",
    "        # Update last gesture time (to apply cooldown for peace sign gesture)\n",
    "        last_gesture_time = time.time()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def detect_thumbs_up(hand_landmarks, margin=0.05):\n",
    "    \n",
    "    # Track the time of the last detected gesture\n",
    "    global last_gesture_time \n",
    "\n",
    "    # Get landmarks\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    middle_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    pinky_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP]\n",
    "    thumb_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP]\n",
    "    \n",
    "    thumb_base = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_CMC]\n",
    "\n",
    "    # Thumb tip should be above other fingertips\n",
    "    thumb_tip_up = ((thumb_tip.y + margin < index_tip.y) and\n",
    "                    (thumb_tip.y + margin < middle_tip.y) and\n",
    "                    (thumb_tip.y + margin < ring_tip.y) and\n",
    "                    (thumb_tip.y + margin < pinky_tip.y) and\n",
    "                    (thumb_tip.y < thumb_mcp.y))\n",
    "    \n",
    "    # Other fingers should be in order from top to bottom\n",
    "    other_fingers_ordered = ((index_mcp.y < middle_mcp.y) and\n",
    "                             (middle_mcp.y < ring_mcp.y) and\n",
    "                             (ring_mcp.y < pinky_mcp.y))\n",
    "    \n",
    "    \n",
    "    if thumb_tip_up and other_fingers_ordered:\n",
    "        \n",
    "        # Update last gesture time (to apply cooldown for thumbs-up gesture)\n",
    "        last_gesture_time = time.time()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_thumbs_down(hand_landmarks, margin=0.05):\n",
    "    \n",
    "    # Track the time of the last detected gesture\n",
    "    global last_gesture_time\n",
    "\n",
    "    # Get landmarks\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    middle_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    pinky_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP]\n",
    "\n",
    "    # Thumb tip must be lower than anything else\n",
    "    thumb_tip_down = ((thumb_tip.y > index_tip.y + margin) and\n",
    "                    (thumb_tip.y > middle_tip.y + margin) and\n",
    "                    (thumb_tip.y > ring_tip.y + margin) and\n",
    "                    (thumb_tip.y > pinky_tip.y + margin))\n",
    "    \n",
    "    # Other fingers should be in the order (from top to down) pinky > ring > middle > index finger\n",
    "    other_fingers_ordered = ((index_mcp.y > middle_mcp.y) and\n",
    "                             (middle_mcp.y > ring_mcp.y) and\n",
    "                             (ring_mcp.y > pinky_mcp.y))\n",
    "    \n",
    "    if thumb_tip_down and other_fingers_ordered:\n",
    "        \n",
    "        # Update last gesture time (to apply cooldown for thumbs-down gesture)\n",
    "        last_gesture_time = time.time()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detect_scroll(hand_landmarks, threshold=0.1, dominance_ratio=4.0):\n",
    "    \n",
    "    # Get previous positions and last gesture time\n",
    "    global previous_thumb_tip, previous_index_tip, last_gesture_time\n",
    "\n",
    "    # Get current positions\n",
    "    current_thumb_tip_x = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].x\n",
    "    current_thumb_tip_y = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y\n",
    "    current_index_tip_x = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x\n",
    "    current_index_tip_y = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y\n",
    "\n",
    "    # Check for cooldown\n",
    "    current_time = time.time()\n",
    "    \n",
    "    # If last gesture happened not before cooldown, return False (no scroll)\n",
    "    if current_time - last_gesture_time < COOLDOWN_PERIOD:\n",
    "        return False, None \n",
    "\n",
    "    # Initialize previous positions if not set\n",
    "    if previous_thumb_tip is None or previous_index_tip is None:\n",
    "        previous_thumb_tip = (current_thumb_tip_x, current_thumb_tip_y)\n",
    "        previous_index_tip = (current_index_tip_x, current_index_tip_y)\n",
    "        return False, None\n",
    "\n",
    "    # Calculate location changes\n",
    "    thumb_horizontal_disp = current_thumb_tip_x - previous_thumb_tip[0]\n",
    "    thumb_vertical_disp = current_thumb_tip_y - previous_thumb_tip[1]\n",
    "    index_horizontal_disp = current_index_tip_x - previous_index_tip[0]\n",
    "    index_vertical_disp = current_index_tip_y - previous_index_tip[1]\n",
    "\n",
    "    # Average the movements of thumb and index for robustness\n",
    "    horizontal_disp = (thumb_horizontal_disp + index_horizontal_disp) / 2\n",
    "    vertical_disp = (thumb_vertical_disp + index_vertical_disp) / 2\n",
    "\n",
    "    # Determine dominant movement (we want to return only a horizontal or vertical movement)\n",
    "    horizontal_movement = abs(horizontal_disp) > threshold\n",
    "    vertical_movement = abs(vertical_disp) > threshold\n",
    "\n",
    "    # Check dominance\n",
    "    if horizontal_movement and abs(horizontal_disp) > dominance_ratio * abs(vertical_disp):\n",
    "        direction = \"right\" if horizontal_disp > 0 else \"left\"\n",
    "        dominant_axis = \"horizontal\"\n",
    "    elif vertical_movement and abs(vertical_disp) > dominance_ratio * abs(horizontal_disp):\n",
    "        direction = \"down\" if vertical_disp > 0 else \"up\"\n",
    "        dominant_axis = \"vertical\"\n",
    "    else:\n",
    "        direction = None\n",
    "        dominant_axis = None\n",
    "\n",
    "    # If there is a dominant movement\n",
    "    if dominant_axis:\n",
    "        \n",
    "        # Update positions\n",
    "        previous_thumb_tip = (current_thumb_tip_x, current_thumb_tip_y)\n",
    "        previous_index_tip = (current_index_tip_x, current_index_tip_y)\n",
    "        \n",
    "        # Update last gesture time\n",
    "        last_gesture_time = current_time \n",
    "        return True, direction\n",
    "\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T16:18:41.742014Z",
     "start_time": "2024-12-07T16:18:19.305338Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733740389.826000   73686 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733740389.893388   74420 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733740389.926177   74420 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-12-09 13:33:10.441 Python[2017:73686] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'Esc' to exit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1733740391.747621   74419 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Global variables for tracking previous positions\n",
    "previous_thumb_tip = None\n",
    "previous_index_tip = None\n",
    "\n",
    "print(\"Press 'Esc' to exit.\")\n",
    "\n",
    "# Initialize gesture display variables\n",
    "current_gesture = None \n",
    "gesture_display_time = 0 \n",
    "GESTURE_DISPLAY_DURATION = 1.5 \n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Detect gestures\n",
    "            detected_gesture = None\n",
    "            if detect_peace_sign(hand_landmarks):\n",
    "                detected_gesture = \"Peace Sign Detected!\"\n",
    "            elif detect_thumbs_up(hand_landmarks):\n",
    "                detected_gesture = \"Thumbs Up Detected!\"\n",
    "            elif detect_thumbs_down(hand_landmarks):\n",
    "                detected_gesture = \"Thumbs Down Detected!\"\n",
    "            \n",
    "            # If a gesture is detected, update the display variables\n",
    "            if detected_gesture:\n",
    "                current_gesture = detected_gesture\n",
    "                gesture_display_time = time.time()\n",
    "            else:\n",
    "                # Only check for scroll if no other gesture is detected\n",
    "                detected, direction = detect_scroll(hand_landmarks, threshold=0.1, dominance_ratio=4.0)\n",
    "                if detected:\n",
    "                    current_gesture = f\"Scroll Detected: {direction.title()}!\"\n",
    "                    gesture_display_time = time.time()\n",
    "\n",
    "    # Display the current gesture if within the display duration\n",
    "    if current_gesture and (time.time() - gesture_display_time < GESTURE_DISPLAY_DURATION):\n",
    "        cv2.putText(frame, current_gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Gesture Recognition', frame)\n",
    "\n",
    "    # Break the loop when 'Esc' key is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cursor control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T13:10:38.815812Z",
     "start_time": "2024-12-07T13:10:27.037631Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733740596.988830   76643 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733740597.024627   77265 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733740597.056736   77265 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-12-09 13:36:37.474 Python[2054:76643] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'Esc' to exit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Get screen dimensions\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "def is_click_gesture(hand_landmarks):\n",
    "    \"\"\"Detect a pinching gesture for a left click.\"\"\"\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "    # Calculate the 3D Euclidean distance between index tip and thumb tip\n",
    "    distance = ((index_tip.x - thumb_tip.x) ** 2 +\n",
    "                (index_tip.y - thumb_tip.y) ** 2 +\n",
    "                (index_tip.z - thumb_tip.z) ** 2) ** 0.5\n",
    "\n",
    "    # Adjust threshold based on typical 3D distances observed\n",
    "    return distance < 0.05\n",
    "\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Press 'Esc' to exit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a mirror-like effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get index finger tip coordinates\n",
    "            index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "            # Normalize coordinates to screen dimensions\n",
    "            cursor_x = int(index_finger_tip.x * screen_width)\n",
    "            cursor_y = int(index_finger_tip.y * screen_height)\n",
    "\n",
    "            # Move the mouse cursor\n",
    "            pyautogui.moveTo(cursor_x, cursor_y)\n",
    "\n",
    "            # Detect click gesture\n",
    "            if is_click_gesture(hand_landmarks):\n",
    "                pyautogui.click()\n",
    "                cv2.putText(frame, \"Click!\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Gesture-Based Cursor Control\", frame)\n",
    "\n",
    "    # Break the loop when 'Esc' key is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk, ImageDraw, ImageFont\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the main Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Gesture-Controlled Instagram Feed\")\n",
    "root.geometry(\"1024x768\")\n",
    "root.configure(bg=\"#f0f0f0\")\n",
    "\n",
    "# Load photos into a list\n",
    "photo_folder = \"photos/\"\n",
    "photo_files = [os.path.join(photo_folder, f) for f in os.listdir(photo_folder) if f.endswith((\".jpg\", \".png\"))]\n",
    "photos = [Image.open(photo).resize((400, 400)) for photo in photo_files]\n",
    "\n",
    "# Create a label to display a single photo\n",
    "photo_label = tk.Label(root, bg=\"#ffffff\", width=400, height=400)\n",
    "photo_label.pack(side=tk.LEFT, padx=10, pady=10)\n",
    "\n",
    "# Feedback label\n",
    "feedback_label = tk.Label(root, text=\"Perform gestures to interact!\", font=(\"Helvetica\", 14), bg=\"#f0f0f0\")\n",
    "feedback_label.pack(side=tk.BOTTOM, pady=20)\n",
    "\n",
    "# Interaction states\n",
    "current_photo_index = 0  # Start with the first photo\n",
    "liked_photos = {}\n",
    "disliked_photos = {}\n",
    "saved_photos = []  # List to store saved photos\n",
    "showing_saved_photos = False  # Track whether we are showing saved photos\n",
    "\n",
    "# Camera feed frame\n",
    "camera_frame = tk.Label(root, bg=\"#000000\", width=500, height=700)\n",
    "camera_frame.pack(side=tk.RIGHT, padx=10, pady=10)\n",
    "\n",
    "# Function to add emoji to photos\n",
    "def add_emoji(photo, emoji):\n",
    "    \"\"\"Add an emoji to the given photo.\"\"\"\n",
    "    img = photo.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 50)  # Ensure you have a compatible font installed\n",
    "    draw.text((150, 150), emoji, fill=\"red\", font=font)\n",
    "    return img\n",
    "\n",
    "# Function to update the displayed photo\n",
    "def update_photo():\n",
    "    \"\"\"Update the currently displayed photo based on the index.\"\"\"\n",
    "    global current_photo_index\n",
    "    if showing_saved_photos:\n",
    "        if len(saved_photos) == 0:\n",
    "            feedback_label.config(text=\"No saved photos available!\")\n",
    "            photo_label.config(image=\"\")\n",
    "            return\n",
    "        if 0 <= current_photo_index < len(saved_photos):\n",
    "            img = ImageTk.PhotoImage(saved_photos[current_photo_index])\n",
    "            photo_label.configure(image=img)\n",
    "            photo_label.img = img  # Keep a reference to avoid garbage collection\n",
    "    else:\n",
    "        if 0 <= current_photo_index < len(photos):\n",
    "            current_photo = photos[current_photo_index]\n",
    "            if current_photo_index in liked_photos:\n",
    "                current_photo = liked_photos[current_photo_index]\n",
    "            elif current_photo_index in disliked_photos:\n",
    "                current_photo = disliked_photos[current_photo_index]\n",
    "            img = ImageTk.PhotoImage(current_photo)\n",
    "            photo_label.configure(image=img)\n",
    "            photo_label.img = img  # Keep a reference to avoid garbage collection\n",
    "\n",
    "# Function to toggle between all photos and saved photos\n",
    "def show_saved_photos():\n",
    "    global showing_saved_photos, current_photo_index\n",
    "    showing_saved_photos = not showing_saved_photos\n",
    "    current_photo_index = 0  # Reset to the first photo\n",
    "    if showing_saved_photos:\n",
    "        feedback_label.config(text=\"Showing Saved Photos 📂\")\n",
    "    else:\n",
    "        feedback_label.config(text=\"Showing All Photos 🌍\")\n",
    "    update_photo()\n",
    "\n",
    "# Gesture Detection Logic\n",
    "def detect_gesture(hand_landmarks):\n",
    "    \"\"\"Detect gestures for liking, disliking, saving, scrolling, and clicking.\"\"\"\n",
    "    global current_photo_index\n",
    "\n",
    "    # Get landmarks for gestures\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "    # Like Gesture (Thumbs Up)\n",
    "    if (\n",
    "        thumb_tip.y < index_tip.y\n",
    "        and thumb_tip.y < middle_tip.y\n",
    "        and thumb_tip.y < ring_tip.y\n",
    "        and thumb_tip.y < pinky_tip.y\n",
    "    ):\n",
    "        feedback_label.config(text=\"Photo Liked! ❤️\")\n",
    "        if current_photo_index not in liked_photos:\n",
    "            liked_photos[current_photo_index] = add_emoji(photos[current_photo_index], \"❤️\")\n",
    "        update_photo()\n",
    "        time.sleep(0.5)\n",
    "        return\n",
    "\n",
    "    # Dislike Gesture (Thumbs Down)\n",
    "    if (\n",
    "        thumb_tip.y > index_tip.y\n",
    "        and thumb_tip.y > middle_tip.y\n",
    "        and thumb_tip.y > ring_tip.y\n",
    "        and thumb_tip.y > pinky_tip.y\n",
    "    ):\n",
    "        feedback_label.config(text=\"Photo Disliked! 👎\")\n",
    "        if current_photo_index not in disliked_photos:\n",
    "            disliked_photos[current_photo_index] = add_emoji(photos[current_photo_index], \"👎\")\n",
    "        update_photo()\n",
    "        time.sleep(0.5)\n",
    "        return\n",
    "\n",
    "    # Save Gesture (Peace Sign)\n",
    "    if (\n",
    "        index_tip.y < middle_tip.y  # Index finger is above middle finger\n",
    "        and middle_tip.y < ring_tip.y  # Middle finger is above ring finger\n",
    "        and ring_tip.y < pinky_tip.y  # Ring finger is above pinky\n",
    "        and thumb_tip.y > index_tip.y  # Thumb is below the index\n",
    "    ):\n",
    "        feedback_label.config(text=\"Photo Saved! ✌️\")\n",
    "        current_photo = photos[current_photo_index]\n",
    "        if current_photo not in saved_photos:\n",
    "            saved_photos.append(current_photo)\n",
    "        time.sleep(0.5)\n",
    "        return\n",
    "\n",
    "    # Scrolling Gesture (Index Finger Movement)\n",
    "    if index_tip.y < hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].y:\n",
    "        feedback_label.config(text=\"Scrolling Up ⬆️\")\n",
    "        if current_photo_index > 0:\n",
    "            current_photo_index -= 1\n",
    "            update_photo()\n",
    "        time.sleep(0.5)\n",
    "    elif index_tip.y > hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].y:\n",
    "        feedback_label.config(text=\"Scrolling Down ⬇️\")\n",
    "        if showing_saved_photos:\n",
    "            if current_photo_index < len(saved_photos) - 1:\n",
    "                current_photo_index += 1\n",
    "                update_photo()\n",
    "        else:\n",
    "            if current_photo_index < len(photos) - 1:\n",
    "                current_photo_index += 1\n",
    "                update_photo()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Open webcam and process gestures\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def update_camera_feed():\n",
    "    \"\"\"Update the camera feed and process gestures in real-time.\"\"\"\n",
    "    global current_photo_index\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return\n",
    "\n",
    "    # Flip the frame horizontally for a mirror-like effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the frame to RGB for MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame for hand landmarks\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Detect gestures for liking, disliking, saving, scrolling, and clicking\n",
    "            detect_gesture(hand_landmarks)\n",
    "\n",
    "    # Convert the frame to an image for Tkinter\n",
    "    frame_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    imgtk = ImageTk.PhotoImage(image=frame_image)\n",
    "\n",
    "    # Display the camera feed in the GUI\n",
    "    camera_frame.imgtk = imgtk\n",
    "    camera_frame.configure(image=imgtk)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    root.after(10, update_camera_feed)\n",
    "\n",
    "# Start with the first photo\n",
    "update_photo()\n",
    "\n",
    "# Add a button to toggle saved photos\n",
    "saved_button = tk.Button(root, text=\"Show Saved Photos\", command=show_saved_photos, font=(\"Helvetica\", 12))\n",
    "saved_button.pack(side=tk.BOTTOM, pady=10)\n",
    "\n",
    "# Start the camera feed\n",
    "update_camera_feed()\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "root.mainloop()\n",
    "\n",
    "# Release the camera\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tetris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bu cursorsız, bi right left yapılmıyor ondan tetris çalışıyo mu düzgün anlayamadım "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733740479.136341   75699 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733740479.188548   75793 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733740479.237855   75793 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-12-09 13:34:39.740 Python[2040:75699] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "W0000 00:00:1733740481.406676   75790 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/tkinter/__init__.py\", line 1967, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/tkinter/__init__.py\", line 861, in callit\n",
      "    func(*args)\n",
      "  File \"/var/folders/0t/47ys7t5x5_xdtktmg5h5m6300000gn/T/ipykernel_2040/707802952.py\", line 221, in update_camera_feed\n",
      "    detect_gesture(hand_landmarks)\n",
      "  File \"/var/folders/0t/47ys7t5x5_xdtktmg5h5m6300000gn/T/ipykernel_2040/707802952.py\", line 196, in detect_gesture\n",
      "    if detect_peace_sign(hand_landmarks):\n",
      "       ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'detect_peace_sign' is not defined\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import random\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Tkinter Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Gesture-Based Tetris\")\n",
    "root.geometry(\"1100x800\")\n",
    "root.configure(bg=\"black\")\n",
    "\n",
    "# Game Variables\n",
    "rows, cols = 20, 10\n",
    "cell_size = 30\n",
    "board = [[0 for _ in range(cols)] for _ in range(rows)]\n",
    "shapes = [\n",
    "    [[1, 1, 1, 1]],  # Line\n",
    "    [[1, 1], [1, 1]],  # Square\n",
    "    [[0, 1, 0], [1, 1, 1]],  # T-shape\n",
    "    [[1, 1, 0], [0, 1, 1]],  # Z-shape\n",
    "    [[0, 1, 1], [1, 1, 0]]   # S-shape\n",
    "]\n",
    "current_shape = None\n",
    "next_shape = random.choice(shapes)  # Initialize next shape\n",
    "current_position = (0, cols // 2 - 1)\n",
    "game_running = False\n",
    "score = 0\n",
    "\n",
    "# Grid Canvas\n",
    "game_canvas = tk.Canvas(root, width=cols * cell_size, height=rows * cell_size, bg=\"black\", highlightthickness=1)\n",
    "game_canvas.pack(side=tk.LEFT, padx=20, pady=20)\n",
    "\n",
    "# Next Shape Canvas\n",
    "next_shape_label = tk.Label(root, text=\"Next Shape:\", font=(\"Helvetica\", 14), bg=\"black\", fg=\"white\")\n",
    "next_shape_label.place(x=cols * cell_size + 150, y=300)  # Positioned next to the grid\n",
    "\n",
    "next_canvas = tk.Canvas(root, width=4 * cell_size, height=4 * cell_size, bg=\"black\", highlightthickness=0)\n",
    "next_canvas.place(x=cols * cell_size + 150, y=330)  # Positioned below the label\n",
    "\n",
    "# Score Label\n",
    "score_label = tk.Label(root, text=f\"Score: {score}\", font=(\"Helvetica\", 18), bg=\"black\", fg=\"white\")\n",
    "score_label.pack(anchor=\"n\", pady=10)\n",
    "\n",
    "# Instructions Label\n",
    "instructions_label = tk.Label(\n",
    "    root,\n",
    "    text=\"Gesture Instructions:\\n✌️: Rotate | 👎: Instant Drop | ⬅️: Move Left | ➡️: Move Right\",\n",
    "    font=(\"Helvetica\", 14),\n",
    "    bg=\"black\",\n",
    "    fg=\"white\",\n",
    "    justify=\"left\"\n",
    ")\n",
    "instructions_label.place(x=cols * cell_size + 150, y=50)\n",
    "\n",
    "# Camera Feed Placeholder\n",
    "camera_label = tk.Label(root, bg=\"black\", width=400, height=400)\n",
    "camera_label.pack(side=tk.RIGHT, padx=10, pady=20)\n",
    "\n",
    "# Start and Stop Buttons\n",
    "def start_game():\n",
    "    global game_running\n",
    "    game_running = True\n",
    "    spawn_shape()\n",
    "    draw_board()\n",
    "    game_loop()\n",
    "\n",
    "def stop_game():\n",
    "    global game_running\n",
    "    game_running = False\n",
    "    score_label.config(text=\"Game Over! Final Score: \" + str(score))\n",
    "\n",
    "start_button = tk.Button(root, text=\"Start Game\", command=start_game, font=(\"Helvetica\", 12), bg=\"green\", fg=\"white\")\n",
    "start_button.place(x=cols * cell_size + 150, y=200)\n",
    "\n",
    "stop_button = tk.Button(root, text=\"Stop Game\", command=stop_game, font=(\"Helvetica\", 12), bg=\"red\", fg=\"white\")\n",
    "stop_button.place(x=cols * cell_size + 150, y=250)\n",
    "\n",
    "# Draw the Game Board\n",
    "def draw_board():\n",
    "    game_canvas.delete(\"all\")\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if board[r][c] != 0:\n",
    "                game_canvas.create_rectangle(\n",
    "                    c * cell_size, r * cell_size, (c + 1) * cell_size, (r + 1) * cell_size,\n",
    "                    fill=\"blue\", outline=\"gray\"\n",
    "                )\n",
    "    if current_shape:\n",
    "        shape, position = current_shape, current_position\n",
    "        for r, row in enumerate(shape):\n",
    "            for c, cell in enumerate(row):\n",
    "                if cell:\n",
    "                    x = position[1] + c\n",
    "                    y = position[0] + r\n",
    "                    if 0 <= y < rows and 0 <= x < cols:\n",
    "                        game_canvas.create_rectangle(\n",
    "                            x * cell_size, y * cell_size, (x + 1) * cell_size, (y + 1) * cell_size,\n",
    "                            fill=\"red\", outline=\"black\"\n",
    "                        )\n",
    "\n",
    "# Draw the Next Shape\n",
    "def draw_next_shape():\n",
    "    next_canvas.delete(\"all\")\n",
    "    for r, row in enumerate(next_shape):\n",
    "        for c, cell in enumerate(row):\n",
    "            if cell:\n",
    "                next_canvas.create_rectangle(\n",
    "                    c * cell_size, r * cell_size, (c + 1) * cell_size, (r + 1) * cell_size,\n",
    "                    fill=\"green\", outline=\"gray\"\n",
    "                )\n",
    "\n",
    "# Spawn New Shape\n",
    "def spawn_shape():\n",
    "    global current_shape, next_shape, current_position\n",
    "    current_shape = next_shape\n",
    "    current_position = (0, cols // 2 - len(current_shape[0]) // 2)\n",
    "    next_shape = random.choice(shapes)\n",
    "    draw_next_shape()\n",
    "    if not can_move(current_shape, current_position):\n",
    "        stop_game()\n",
    "\n",
    "# Check Valid Move\n",
    "def can_move(shape, position):\n",
    "    for r, row in enumerate(shape):\n",
    "        for c, cell in enumerate(row):\n",
    "            if cell:\n",
    "                x = position[1] + c\n",
    "                y = position[0] + r\n",
    "                if x < 0 or x >= cols or y >= rows or (y >= 0 and board[y][x] != 0):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "# Place Shape on Board\n",
    "def place_shape():\n",
    "    global board, score, current_shape, current_position\n",
    "    shape, position = current_shape, current_position\n",
    "    for r, row in enumerate(shape):\n",
    "        for c, cell in enumerate(row):\n",
    "            if cell:\n",
    "                x = position[1] + c\n",
    "                y = position[0] + r\n",
    "                board[y][x] = 1\n",
    "    clear_rows()\n",
    "    spawn_shape()\n",
    "\n",
    "# Clear Completed Rows\n",
    "def clear_rows():\n",
    "    global board, score\n",
    "    new_board = [row for row in board if any(cell == 0 for cell in row)]\n",
    "    rows_cleared = rows - len(new_board)\n",
    "    board = [[0 for _ in range(cols)] for _ in range(rows_cleared)] + new_board\n",
    "    score += rows_cleared * 100\n",
    "    score_label.config(text=f\"Score: {score}\")\n",
    "\n",
    "# Move Shape Down\n",
    "def move_down():\n",
    "    global current_position\n",
    "    new_position = (current_position[0] + 1, current_position[1])\n",
    "    if can_move(current_shape, new_position):\n",
    "        current_position = new_position\n",
    "    else:\n",
    "        place_shape()\n",
    "\n",
    "# Move Shape Horizontally\n",
    "def move_shape(dx):\n",
    "    global current_position\n",
    "    new_position = (current_position[0], current_position[1] + dx)\n",
    "    if can_move(current_shape, new_position):\n",
    "        current_position = new_position\n",
    "\n",
    "# Rotate Shape\n",
    "def rotate_shape():\n",
    "    global current_shape\n",
    "    rotated = [[current_shape[r][c] for r in range(len(current_shape))] for c in range(len(current_shape[0]) - 1, -1, -1)]\n",
    "    if can_move(rotated, current_position):\n",
    "        current_shape = rotated\n",
    "\n",
    "def instant_drop():\n",
    "    global current_position\n",
    "    # Faster drop rate: reduce the time interval between drops\n",
    "    while can_move(current_shape, (current_position[0] + 1, current_position[1])):\n",
    "        current_position = (current_position[0] + 1, current_position[1])\n",
    "        draw_board()\n",
    "        root.update()  # Update the UI to visually show the faster movement\n",
    "        root.after(50)  # Adjust this value for desired speed (lower is faster)\n",
    "    place_shape()\n",
    "\n",
    "# Gesture Detection\n",
    "def detect_gesture(hand_landmarks):\n",
    "    if detect_peace_sign(hand_landmarks):\n",
    "        rotate_shape()\n",
    "        instructions_label.config(text=\"Peace Gesture ✌️ Rotate!\")\n",
    "    elif detect_thumbs_down(hand_landmarks):\n",
    "        instant_drop()\n",
    "        instructions_label.config(text=\"Thumbs Down 👎 Instant Drop!\")\n",
    "    scroll_direction = detect_scroll(hand_landmarks)\n",
    "    if scroll_direction == \"right\":\n",
    "        move_shape(1)\n",
    "        instructions_label.config(text=\"Scroll ➡️ Move Right!\")\n",
    "    elif scroll_direction == \"left\":\n",
    "        move_shape(-1)\n",
    "        instructions_label.config(text=\"Scroll ⬅️ Move Left!\")\n",
    "\n",
    "# Update Camera Feed\n",
    "def update_camera_feed():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            detect_gesture(hand_landmarks)\n",
    "    frame_image = Image.fromarray(frame_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=frame_image)\n",
    "    camera_label.imgtk = imgtk\n",
    "    camera_label.configure(image=imgtk)\n",
    "    root.after(10, update_camera_feed)\n",
    "\n",
    "# Game Loop\n",
    "def game_loop():\n",
    "    if game_running:\n",
    "        move_down()\n",
    "        draw_board()\n",
    "        root.after(500, game_loop)\n",
    "\n",
    "# Start the Game\n",
    "cap = cv2.VideoCapture(0)\n",
    "spawn_shape()\n",
    "draw_board()\n",
    "update_camera_feed()\n",
    "root.mainloop()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bu cursorlu gesture falan algılamıyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating camera and game...\n",
      "Camera not working!\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import random\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from PIL import Image, ImageTk\n",
    "import pyautogui\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Tkinter Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Gesture-Based Tetris\")\n",
    "root.geometry(\"1100x800\")\n",
    "root.configure(bg=\"black\")\n",
    "\n",
    "# Game Variables\n",
    "rows, cols = 20, 10\n",
    "cell_size = 30\n",
    "board = [[0 for _ in range(cols)] for _ in range(rows)]\n",
    "shapes = [\n",
    "    [[1, 1, 1, 1]],  # Line\n",
    "    [[1, 1], [1, 1]],  # Square\n",
    "    [[0, 1, 0], [1, 1, 1]],  # T-shape\n",
    "    [[1, 1, 0], [0, 1, 1]],  # Z-shape\n",
    "    [[0, 1, 1], [1, 1, 0]]   # S-shape\n",
    "]\n",
    "current_shape = None\n",
    "next_shape = random.choice(shapes)  # Initialize next shape\n",
    "current_position = (0, cols // 2 - 1)\n",
    "game_running = False\n",
    "score = 0\n",
    "\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Grid Canvas\n",
    "game_canvas = tk.Canvas(root, width=cols * cell_size, height=rows * cell_size, bg=\"black\", highlightthickness=1)\n",
    "game_canvas.pack(side=tk.LEFT, padx=20, pady=20)\n",
    "\n",
    "# Next Shape Canvas\n",
    "next_shape_label = tk.Label(root, text=\"Next Shape:\", font=(\"Helvetica\", 14), bg=\"black\", fg=\"white\")\n",
    "next_shape_label.place(x=cols * cell_size + 150, y=300)  # Positioned next to the grid\n",
    "\n",
    "next_canvas = tk.Canvas(root, width=4 * cell_size, height=4 * cell_size, bg=\"black\", highlightthickness=0)\n",
    "next_canvas.place(x=cols * cell_size + 150, y=330)  # Positioned below the label\n",
    "\n",
    "# Score Label\n",
    "score_label = tk.Label(root, text=f\"Score: {score}\", font=(\"Helvetica\", 18), bg=\"black\", fg=\"white\")\n",
    "score_label.pack(anchor=\"n\", pady=10)\n",
    "\n",
    "# Instructions Label\n",
    "instructions_label = tk.Label(\n",
    "    root,\n",
    "    text=\"Gesture Instructions:\\n✌️: Rotate | 👎: Instant Drop | ⬅️: Move Left | ➡️: Move Right\",\n",
    "    font=(\"Helvetica\", 14),\n",
    "    bg=\"black\",\n",
    "    fg=\"white\",\n",
    "    justify=\"left\"\n",
    ")\n",
    "instructions_label.place(x=cols * cell_size + 150, y=50)\n",
    "\n",
    "# Camera Feed Placeholder\n",
    "camera_label = tk.Label(root, bg=\"black\", width=400, height=400)\n",
    "camera_label.pack(side=tk.RIGHT, padx=10, pady=20)\n",
    "\n",
    "# Start and Stop Buttons\n",
    "def start_game():\n",
    "    global game_running\n",
    "    game_running = True\n",
    "    spawn_shape()\n",
    "    draw_board()\n",
    "    game_loop()\n",
    "\n",
    "def stop_game():\n",
    "    global game_running\n",
    "    game_running = False\n",
    "    score_label.config(text=\"Game Over! Final Score: \" + str(score))\n",
    "\n",
    "start_button = tk.Button(root, text=\"Start Game\", command=start_game, font=(\"Helvetica\", 12), bg=\"green\", fg=\"white\")\n",
    "start_button.place(x=cols * cell_size + 150, y=200)\n",
    "\n",
    "stop_button = tk.Button(root, text=\"Stop Game\", command=stop_game, font=(\"Helvetica\", 12), bg=\"red\", fg=\"white\")\n",
    "stop_button.place(x=cols * cell_size + 150, y=250)\n",
    "\n",
    "# Draw the Game Board\n",
    "def draw_board():\n",
    "    game_canvas.delete(\"all\")\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if board[r][c] != 0:\n",
    "                game_canvas.create_rectangle(\n",
    "                    c * cell_size, r * cell_size, (c + 1) * cell_size, (r + 1) * cell_size,\n",
    "                    fill=\"blue\", outline=\"gray\"\n",
    "                )\n",
    "    if current_shape:\n",
    "        shape, position = current_shape, current_position\n",
    "        for r, row in enumerate(shape):\n",
    "            for c, cell in enumerate(row):\n",
    "                if cell:\n",
    "                    x = position[1] + c\n",
    "                    y = position[0] + r\n",
    "                    if 0 <= y < rows and 0 <= x < cols:\n",
    "                        game_canvas.create_rectangle(\n",
    "                            x * cell_size, y * cell_size, (x + 1) * cell_size, (y + 1) * cell_size,\n",
    "                            fill=\"red\", outline=\"black\"\n",
    "                        )\n",
    "\n",
    "# Draw the Next Shape\n",
    "def draw_next_shape():\n",
    "    next_canvas.delete(\"all\")\n",
    "    for r, row in enumerate(next_shape):\n",
    "        for c, cell in enumerate(row):\n",
    "            if cell:\n",
    "                next_canvas.create_rectangle(\n",
    "                    c * cell_size, r * cell_size, (c + 1) * cell_size, (r + 1) * cell_size,\n",
    "                    fill=\"green\", outline=\"gray\"\n",
    "                )\n",
    "\n",
    "# Spawn New Shape\n",
    "def spawn_shape():\n",
    "    global current_shape, next_shape, current_position\n",
    "    current_shape = next_shape\n",
    "    current_position = (0, cols // 2 - len(current_shape[0]) // 2)\n",
    "    next_shape = random.choice(shapes)\n",
    "    draw_next_shape()\n",
    "    if not can_move(current_shape, current_position):\n",
    "        stop_game()\n",
    "\n",
    "# Check Valid Move\n",
    "def can_move(shape, position):\n",
    "    for r, row in enumerate(shape):\n",
    "        for c, cell in enumerate(row):\n",
    "            if cell:\n",
    "                x = position[1] + c\n",
    "                y = position[0] + r\n",
    "                if x < 0 or x >= cols or y >= rows or (y >= 0 and board[y][x] != 0):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "# Place Shape on Board\n",
    "def place_shape():\n",
    "    global board, score, current_shape, current_position\n",
    "    shape, position = current_shape, current_position\n",
    "    for r, row in enumerate(shape):\n",
    "        for c, cell in enumerate(row):\n",
    "            if cell:\n",
    "                x = position[1] + c\n",
    "                y = position[0] + r\n",
    "                board[y][x] = 1\n",
    "    clear_rows()\n",
    "    spawn_shape()\n",
    "\n",
    "# Clear Completed Rows\n",
    "def clear_rows():\n",
    "    global board, score\n",
    "    new_board = [row for row in board if any(cell == 0 for cell in row)]\n",
    "    rows_cleared = rows - len(new_board)\n",
    "    board = [[0 for _ in range(cols)] for _ in range(rows_cleared)] + new_board\n",
    "    score += rows_cleared * 100\n",
    "    score_label.config(text=f\"Score: {score}\")\n",
    "\n",
    "# Move Shape Down\n",
    "def move_down():\n",
    "    global current_position\n",
    "    new_position = (current_position[0] + 1, current_position[1])\n",
    "    if can_move(current_shape, new_position):\n",
    "        current_position = new_position\n",
    "    else:\n",
    "        place_shape()\n",
    "\n",
    "# Move Shape Horizontally\n",
    "def move_shape(dx):\n",
    "    global current_position\n",
    "    new_position = (current_position[0], current_position[1] + dx)\n",
    "    if can_move(current_shape, new_position):\n",
    "        current_position = new_position\n",
    "\n",
    "# Rotate Shape\n",
    "def rotate_shape():\n",
    "    global current_shape\n",
    "    rotated = [[current_shape[r][c] for r in range(len(current_shape))] for c in range(len(current_shape[0]) - 1, -1, -1)]\n",
    "    if can_move(rotated, current_position):\n",
    "        current_shape = rotated\n",
    "\n",
    "def instant_drop():\n",
    "    global current_position\n",
    "    # Faster drop rate: reduce the time interval between drops\n",
    "    while can_move(current_shape, (current_position[0] + 1, current_position[1])):\n",
    "        current_position = (current_position[0] + 1, current_position[1])\n",
    "        draw_board()\n",
    "        root.update()  # Update the UI to visually show the faster movement\n",
    "        root.after(50)  # Adjust this value for desired speed (lower is faster)\n",
    "    place_shape()\n",
    "\n",
    "# Gesture Detection\n",
    "def detect_gesture(hand_landmarks):\n",
    "    if detect_peace_sign(hand_landmarks):\n",
    "        rotate_shape()\n",
    "        instructions_label.config(text=\"Peace Gesture ✌️ Rotate!\")\n",
    "    elif detect_thumbs_down(hand_landmarks):\n",
    "        instant_drop()\n",
    "        instructions_label.config(text=\"Thumbs Down 👎 Instant Drop!\")\n",
    "    scroll_direction = detect_scroll(hand_landmarks)\n",
    "    if scroll_direction == \"right\":\n",
    "        move_shape(1)\n",
    "        instructions_label.config(text=\"Scroll ➡️ Move Right!\")\n",
    "    elif scroll_direction == \"left\":\n",
    "        move_shape(-1)\n",
    "        instructions_label.config(text=\"Scroll ⬅️ Move Left!\")\n",
    "\n",
    "def detect_click_gesture(hand_landmarks):\n",
    "    \"\"\"Detect a pinching gesture for a click.\"\"\"\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "    # Calculate Euclidean distance between the tips\n",
    "    distance = ((index_tip.x - thumb_tip.x) ** 2 +\n",
    "                (index_tip.y - thumb_tip.y) ** 2 +\n",
    "                (index_tip.z - thumb_tip.z) ** 2) ** 0.5\n",
    "\n",
    "    return distance < 0.05\n",
    "\n",
    "def update_cursor_position(hand_landmarks):\n",
    "    \"\"\"Move the cursor based on the index fingertip.\"\"\"\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "    # Map hand coordinates to screen coordinates\n",
    "    cursor_x = int(index_tip.x * screen_width)\n",
    "    cursor_y = int(index_tip.y * screen_height)\n",
    "\n",
    "    # Move cursor\n",
    "    pyautogui.moveTo(cursor_x, cursor_y)\n",
    "\n",
    "# Add this function near other game logic functions\n",
    "def update_camera_and_game():\n",
    "    global game_running, current_position\n",
    "    print(\"Updating camera and game...\")  # Debug print\n",
    "\n",
    "    # Camera feed logic\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Camera not working!\")  # Debug print\n",
    "        return\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            print(\"Hand landmarks detected!\")  # Debug print\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            detect_gesture(hand_landmarks)\n",
    "\n",
    "    # Game logic\n",
    "    if game_running:\n",
    "        print(\"Game is running!\")  # Debug print\n",
    "        move_down()\n",
    "        draw_board()\n",
    "\n",
    "    # Update the camera feed in the UI\n",
    "    frame_image = Image.fromarray(frame_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=frame_image)\n",
    "    camera_label.imgtk = imgtk\n",
    "    camera_label.configure(image=imgtk)\n",
    "\n",
    "    # Schedule the next update\n",
    "    root.after(50, update_camera_and_game)\n",
    "\n",
    "\n",
    "# Call this function instead of `update_camera_feed()` or `game_loop()`\n",
    "update_camera_and_game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733740648.673357   76643 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine\n",
      "W0000 00:00:1733740648.772987   77851 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733740648.833991   77851 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'Esc' to exit.\n"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Screen dimensions for cursor mapping\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Button positions and dimensions\n",
    "BUTTON_WIDTH = 150\n",
    "BUTTON_HEIGHT = 70\n",
    "pause_button = {\"x\": 100, \"y\": 100, \"width\": BUTTON_WIDTH, \"height\": BUTTON_HEIGHT}\n",
    "start_button = {\"x\": 300, \"y\": 100, \"width\": BUTTON_WIDTH, \"height\": BUTTON_HEIGHT}\n",
    "\n",
    "# Game states\n",
    "game_paused = False\n",
    "game_started = False\n",
    "\n",
    "# Gesture detection for click\n",
    "def is_click_gesture(hand_landmarks):\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "    # Calculate the 3D Euclidean distance between index tip and thumb tip\n",
    "    distance = ((index_tip.x - thumb_tip.x) ** 2 +\n",
    "                (index_tip.y - thumb_tip.y) ** 2 +\n",
    "                (index_tip.z - thumb_tip.z) ** 2) ** 0.5\n",
    "\n",
    "    return distance < 0.05\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Press 'Esc' to exit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a mirror-like effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    # Draw buttons on the frame\n",
    "    cv2.rectangle(frame, (pause_button[\"x\"], pause_button[\"y\"]),\n",
    "                  (pause_button[\"x\"] + pause_button[\"width\"], pause_button[\"y\"] + pause_button[\"height\"]),\n",
    "                  (0, 255, 0) if not game_paused else (255, 0, 0), -1)\n",
    "    cv2.putText(frame, \"Pause\", (pause_button[\"x\"] + 20, pause_button[\"y\"] + 45),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.rectangle(frame, (start_button[\"x\"], start_button[\"y\"]),\n",
    "                  (start_button[\"x\"] + start_button[\"width\"], start_button[\"y\"] + start_button[\"height\"]),\n",
    "                  (0, 255, 0) if game_started else (255, 0, 0), -1)\n",
    "    cv2.putText(frame, \"Start\", (start_button[\"x\"] + 20, start_button[\"y\"] + 45),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get index finger tip coordinates for cursor control\n",
    "            index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "            # Normalize coordinates to screen dimensions\n",
    "            cursor_x = int(index_finger_tip.x * screen_width)\n",
    "            cursor_y = int(index_finger_tip.y * screen_height)\n",
    "\n",
    "            # Move the mouse cursor\n",
    "            pyautogui.moveTo(cursor_x, cursor_y)\n",
    "\n",
    "            # Detect click gesture\n",
    "            if is_click_gesture(hand_landmarks):\n",
    "                # Map cursor to the frame dimensions\n",
    "                cursor_on_frame_x = int(index_finger_tip.x * frame_width)\n",
    "                cursor_on_frame_y = int(index_finger_tip.y * frame_height)\n",
    "\n",
    "                # Check if click is on Pause button\n",
    "                if (pause_button[\"x\"] <= cursor_on_frame_x <= pause_button[\"x\"] + pause_button[\"width\"] and\n",
    "                        pause_button[\"y\"] <= cursor_on_frame_y <= pause_button[\"y\"] + pause_button[\"height\"]):\n",
    "                    game_paused = not game_paused\n",
    "                    print(\"Game Paused\" if game_paused else \"Game Resumed\")\n",
    "\n",
    "                # Check if click is on Start button\n",
    "                if (start_button[\"x\"] <= cursor_on_frame_x <= start_button[\"x\"] + start_button[\"width\"] and\n",
    "                        start_button[\"y\"] <= cursor_on_frame_y <= start_button[\"y\"] + start_button[\"height\"]):\n",
    "                    game_started = not game_started\n",
    "                    print(\"Game Started\" if game_started else \"Game Stopped\")\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Gesture-Based Game Control\", frame)\n",
    "\n",
    "    # Break the loop when 'Esc' key is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RAjfOu8vbX9T",
    "5WqLvhGEczPc",
    "AnVjzQPedaun"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
