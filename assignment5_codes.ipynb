{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1hJcrtQbYOs"
   },
   "source": [
    "# HAND GESTURE-BASED INTERACTION\n",
    "\n",
    "---\n",
    "\n",
    "Group members:\n",
    "*   Ada Yƒ±lmaz\n",
    "*   Ceren ≈ûahin\n",
    "*   Sima Adleyba\n",
    "*   Selen Naz G√ºrsoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAjfOu8vbX9T"
   },
   "source": [
    "### Installing necessary libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:12:56.300480Z",
     "start_time": "2024-12-10T14:12:53.525799Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUn2JcSgZ7Q7",
    "outputId": "9c342d67-09a3-467c-8af2-efcfc6585f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install mediapipe\n",
    "%pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:13:01.515541Z",
     "start_time": "2024-12-10T14:12:58.512798Z"
    },
    "id": "KtyLGHBsaCoN"
   },
   "outputs": [],
   "source": [
    "#download a model that can recognize 7 hand gestures: üëç, üëé, ‚úåÔ∏è, ‚òùÔ∏è, ‚úä, üëã, ü§ü\n",
    "!wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:13:07.970931Z",
     "start_time": "2024-12-10T14:13:06.204558Z"
    },
    "id": "j03y4pVScZr5"
   },
   "outputs": [],
   "source": [
    "#download test images from pixabay\n",
    "import urllib\n",
    "\n",
    "IMAGE_FILENAMES = ['thumbs_down.jpg', 'victory.jpg', 'thumbs_up.jpg', 'pointing_up.jpg']\n",
    "\n",
    "for name in IMAGE_FILENAMES:\n",
    "  url = f'https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/{name}'\n",
    "  urllib.request.urlretrieve(url, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:13:10.203930Z",
     "start_time": "2024-12-10T14:13:10.138598Z"
    },
    "id": "a_QNcKXCdAuq"
   },
   "outputs": [],
   "source": [
    "#or we can use our own images as shown below\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for filename in uploaded:\n",
    "#   content = uploaded[filename]\n",
    "#   with open(filename, 'wb') as f:\n",
    "#     f.write(content)\n",
    "# IMAGE_FILENAMES = list(uploaded.keys())\n",
    "\n",
    "# print('Uploaded files:', IMAGE_FILENAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WqLvhGEczPc"
   },
   "source": [
    "### Functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:13:44.357713Z",
     "start_time": "2024-12-10T14:13:18.864770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 17:13:38.461106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:15:02.463209Z",
     "start_time": "2024-12-10T14:15:02.357136Z"
    },
    "id": "eZQdCS6uaRny"
   },
   "outputs": [],
   "source": [
    "#some functions to visualize the gesture recognition results.\n",
    "import math\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'xtick.labelbottom': False,\n",
    "    'xtick.bottom': False,\n",
    "    'ytick.labelleft': False,\n",
    "    'ytick.left': False,\n",
    "    'xtick.labeltop': False,\n",
    "    'xtick.top': False,\n",
    "    'ytick.labelright': False,\n",
    "    'ytick.right': False\n",
    "})\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "\n",
    "def display_one_image(image, title, subplot, titlesize=16):\n",
    "    \"\"\"Displays one image along with the predicted category name and score.\"\"\"\n",
    "    plt.subplot(*subplot)\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "\n",
    "\n",
    "def display_batch_of_images_with_gestures_and_hand_landmarks(images, results):\n",
    "    \"\"\"Displays a batch of images with the gesture category and its score along with the hand landmarks.\"\"\"\n",
    "    # Images and labels.\n",
    "    images = [image.numpy_view() for image in images]\n",
    "    gestures = [top_gesture for (top_gesture, _) in results]\n",
    "    multi_hand_landmarks_list = [multi_hand_landmarks for (_, multi_hand_landmarks) in results]\n",
    "\n",
    "    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images) // rows\n",
    "\n",
    "    # Size and spacing.\n",
    "    FIGSIZE = 13.0\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols, 1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "\n",
    "    # Display gestures and hand landmarks.\n",
    "    for i, (image, gestures) in enumerate(zip(images[:rows*cols], gestures[:rows*cols])):\n",
    "        title = f\"{gestures.category_name} ({gestures.score:.2f})\"\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3\n",
    "        annotated_image = image.copy()\n",
    "\n",
    "        for hand_landmarks in multi_hand_landmarks_list[i]:\n",
    "          hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "          hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "          ])\n",
    "\n",
    "          mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        subplot = display_one_image(annotated_image, title, subplot, titlesize=dynamic_titlesize)\n",
    "\n",
    "    # Layout.\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnVjzQPedaun"
   },
   "source": [
    "### Preview the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:15:13.370737Z",
     "start_time": "2024-12-10T14:15:09.055415Z"
    },
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m9uzHARZddDK",
    "outputId": "42c11584-885e-4a08-ee4c-a81189d64573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying: thumbs_down.jpg\n",
      "Displaying: victory.jpg\n",
      "Displaying: thumbs_up.jpg\n",
      "Displaying: pointing_up.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "\n",
    "DESIRED_HEIGHT = 480\n",
    "DESIRED_WIDTH = 480\n",
    "\n",
    "def resize_and_show(image, name):\n",
    "    h, w = image.shape[:2]\n",
    "    if h < w:\n",
    "        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h / (w / DESIRED_WIDTH))))\n",
    "    else:\n",
    "        img = cv2.resize(image, (math.floor(w / (h / DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
    "    \n",
    "    # Display the image in a window with a name\n",
    "    cv2.imshow(name, img)\n",
    "    cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "    cv2.destroyAllWindows()  # Close the window after key press\n",
    "\n",
    "# Example usage\n",
    "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
    "\n",
    "for name, image in images.items():\n",
    "    if image is not None:\n",
    "        print(f\"Displaying: {name}\")\n",
    "        resize_and_show(image, name)\n",
    "    else:\n",
    "        print(f\"Error: Could not read image {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zRNOZm7d5Wr"
   },
   "source": [
    "### Google implementation - do not run\n",
    "This first one is a how to from google, we can create our own by following the steps given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T12:15:15.574996Z",
     "start_time": "2024-12-07T12:15:15.189956Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JwAdQ_VjeAku",
    "outputId": "f8982a23-7eff-456b-b915-af0daf1ec27f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733734701.863207   14037 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine\n",
      "W0000 00:00:1733734701.909597   14037 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1733734701.932447   14037 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733734702.018345   15212 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733734702.089356   15212 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733734702.094463   15215 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733734702.094618   15215 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733734702.162484   15211 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input image must contain three channel bgr data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m   hand_landmarks \u001b[38;5;241m=\u001b[39m recognition_result\u001b[38;5;241m.\u001b[39mhand_landmarks\n\u001b[1;32m     25\u001b[0m   results\u001b[38;5;241m.\u001b[39mappend((top_gesture, hand_landmarks))\n\u001b[0;32m---> 27\u001b[0m \u001b[43mdisplay_batch_of_images_with_gestures_and_hand_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 68\u001b[0m, in \u001b[0;36mdisplay_batch_of_images_with_gestures_and_hand_landmarks\u001b[0;34m(images, results)\u001b[0m\n\u001b[1;32m     63\u001b[0m       hand_landmarks_proto \u001b[38;5;241m=\u001b[39m landmark_pb2\u001b[38;5;241m.\u001b[39mNormalizedLandmarkList()\n\u001b[1;32m     64\u001b[0m       hand_landmarks_proto\u001b[38;5;241m.\u001b[39mlandmark\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m     65\u001b[0m         landmark_pb2\u001b[38;5;241m.\u001b[39mNormalizedLandmark(x\u001b[38;5;241m=\u001b[39mlandmark\u001b[38;5;241m.\u001b[39mx, y\u001b[38;5;241m=\u001b[39mlandmark\u001b[38;5;241m.\u001b[39my, z\u001b[38;5;241m=\u001b[39mlandmark\u001b[38;5;241m.\u001b[39mz) \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m hand_landmarks\n\u001b[1;32m     66\u001b[0m       ])\n\u001b[0;32m---> 68\u001b[0m       \u001b[43mmp_drawing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_landmarks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mannotated_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhand_landmarks_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmp_hands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHAND_CONNECTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmp_drawing_styles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_hand_landmarks_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmp_drawing_styles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_hand_connections_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     subplot \u001b[38;5;241m=\u001b[39m display_one_image(annotated_image, title, subplot, titlesize\u001b[38;5;241m=\u001b[39mdynamic_titlesize)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Layout.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/mediapipe/python/solutions/drawing_utils.py:158\u001b[0m, in \u001b[0;36mdraw_landmarks\u001b[0;34m(image, landmark_list, connections, landmark_drawing_spec, connection_drawing_spec, is_drawing_landmarks)\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m _BGR_CHANNELS:\n\u001b[0;32m--> 158\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel bgr data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    159\u001b[0m image_rows, image_cols, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    160\u001b[0m idx_to_coordinates \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Input image must contain three channel bgr data."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1300x1300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an GestureRecognizer object.\n",
    "base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')\n",
    "options = vision.GestureRecognizerOptions(base_options=base_options)\n",
    "recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "\n",
    "images = []\n",
    "results = []\n",
    "for image_file_name in IMAGE_FILENAMES:\n",
    "    \n",
    "  # STEP 3: Load the input image.\n",
    "  image = mp.Image.create_from_file(image_file_name)\n",
    "\n",
    "  # STEP 4: Recognize gestures in the input image.\n",
    "  recognition_result = recognizer.recognize(image)\n",
    "\n",
    "  # STEP 5: Process the result. In this case, visualize it.\n",
    "  images.append(image)\n",
    "  top_gesture = recognition_result.gestures[0][0]\n",
    "  hand_landmarks = recognition_result.hand_landmarks\n",
    "  results.append((top_gesture, hand_landmarks))\n",
    "\n",
    "display_batch_of_images_with_gestures_and_hand_landmarks(images, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Example - do not run\n",
    "Tried an example to see how it works and update the functions accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T12:11:34.835600Z",
     "start_time": "2024-12-07T12:11:33.832301Z"
    },
    "id": "vxV_DPPWeBOo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733734784.299569   14037 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine\n",
      "W0000 00:00:1733734784.420640   16680 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733734784.470914   16680 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-12-09 11:59:44.968 Python[1038:14037] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n",
      "Peace sign gesture detected!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def detect_peace_sign(hand_landmarks):\n",
    "    # Get coordinates of relevant landmarks\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "    \n",
    "    \n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    middle_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "    ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    pinky_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP]\n",
    "\n",
    "\n",
    "    # Define peace sign gesture: index and middle fingers up, ring and pinky fingers down\n",
    "    if (\n",
    "        # Index and middle fingers' tips must be highest parts of those fingers\n",
    "        (index_tip.y < index_mcp.y) and (middle_tip.y < middle_mcp.y)\n",
    "        and\n",
    "        # Index finger's tips must be higher than other fingers' mcp\n",
    "        (index_tip.y < ring_mcp.y) and (index_tip.y < pinky_mcp.y)\n",
    "        and\n",
    "        # Middle finger's tips must be higher than other fingers' mcp\n",
    "        (middle_tip.y < ring_mcp.y) and (middle_tip.y < pinky_mcp.y)\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Detect custom gesture (peace sign)\n",
    "            if detect_peace_sign(hand_landmarks):\n",
    "                cv2.putText(frame, 'Peace Sign Detected!', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                # Trigger interaction (e.g., print a message)\n",
    "                print(\"Peace sign gesture detected!\")\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('MediaPipe Hands', frame)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Gesture Detection Functions\n",
    "After checking how the upper ones worked, we implemented fixed, more robust versions of gesture detection functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:15:22.247792Z",
     "start_time": "2024-12-10T14:15:22.093456Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "class GestureApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.geometry(\"800x600\")\n",
    "        self.root.title(\"Gesture Controlled App\")\n",
    "\n",
    "        # Placeholder image\n",
    "        self.image_index = 0\n",
    "        self.images = [f\"image_{i}.jpg\" for i in range(1, 4)]  # Example image names\n",
    "        self.image_label = tk.Label(root)\n",
    "        self.image_label.pack()\n",
    "\n",
    "        # Buttons\n",
    "        self.like_button = tk.Button(root, text=\"Like\", command=self.like_picture)\n",
    "        self.like_button.pack(side=tk.LEFT)\n",
    "\n",
    "        self.dislike_button = tk.Button(root, text=\"Dislike\", command=self.dislike_picture)\n",
    "        self.dislike_button.pack(side=tk.LEFT)\n",
    "\n",
    "        # Camera feed\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.update_camera_feed()\n",
    "\n",
    "    def update_camera_feed(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            print(\"Camera not working!\")\n",
    "            return\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Process hand landmarks here\n",
    "                pass\n",
    "\n",
    "        # Convert the frame to an image format that Tkinter can use\n",
    "        img = Image.fromarray(rgb_frame)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        self.image_label.imgtk = imgtk\n",
    "        self.image_label.configure(image=imgtk)\n",
    "\n",
    "        # Call this method again after 10 milliseconds\n",
    "        self.root.after(10, self.update_camera_feed)\n",
    "\n",
    "    def like_picture(self):\n",
    "        print(\"Liked picture\")\n",
    "\n",
    "    def dislike_picture(self):\n",
    "        print(\"Disliked picture\")\n",
    "\n",
    "# Initialize MediaPipe hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Create the Tkinter window and run the app\n",
    "root = tk.Tk()\n",
    "app = GestureApp(root)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:15:59.859426Z",
     "start_time": "2024-12-10T14:15:47.239564Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733840147.281885  123659 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine\n",
      "W0000 00:00:1733840147.360739  128637 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733840147.423230  128637 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-12-10 17:15:48.769 Python[3423:123659] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'Esc' to exit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1733840153.492748  128634 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Global variables for tracking previous positions\n",
    "previous_thumb_tip = None\n",
    "previous_index_tip = None\n",
    "\n",
    "print(\"Press 'Esc' to exit.\")\n",
    "\n",
    "# Initialize gesture display variables\n",
    "current_gesture = None \n",
    "gesture_display_time = 0 \n",
    "GESTURE_DISPLAY_DURATION = 1.5 \n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Detect gestures\n",
    "            detected_gesture = None\n",
    "            if detect_peace_sign(hand_landmarks):\n",
    "                detected_gesture = \"Peace Sign Detected!\"\n",
    "            elif detect_thumbs_up(hand_landmarks):\n",
    "                detected_gesture = \"Thumbs Up Detected!\"\n",
    "            elif detect_thumbs_down(hand_landmarks):\n",
    "                detected_gesture = \"Thumbs Down Detected!\"\n",
    "            \n",
    "            # If a gesture is detected, update the display variables\n",
    "            if detected_gesture:\n",
    "                current_gesture = detected_gesture\n",
    "                gesture_display_time = time.time()\n",
    "            else:\n",
    "                # Only check for scroll if no other gesture is detected\n",
    "                detected, direction = detect_scroll(hand_landmarks, threshold=0.1, dominance_ratio=4.0)\n",
    "                if detected:\n",
    "                    current_gesture = f\"Scroll Detected: {direction.title()}!\"\n",
    "                    gesture_display_time = time.time()\n",
    "\n",
    "    # Display the current gesture if within the display duration\n",
    "    if current_gesture and (time.time() - gesture_display_time < GESTURE_DISPLAY_DURATION):\n",
    "        cv2.putText(frame, current_gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Gesture Recognition', frame)\n",
    "\n",
    "    # Break the loop when 'Esc' key is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cursor control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T13:10:38.815812Z",
     "start_time": "2024-12-07T13:10:27.037631Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733740596.988830   76643 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733740597.024627   77265 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733740597.056736   77265 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-12-09 13:36:37.474 Python[2054:76643] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'Esc' to exit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Get screen dimensions\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "def is_click_gesture(hand_landmarks):\n",
    "    \"\"\"Detect a pinching gesture for a left click.\"\"\"\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "\n",
    "    # Calculate the 3D Euclidean distance between index tip and thumb tip\n",
    "    distance = ((index_tip.x - thumb_tip.x) ** 2 +\n",
    "                (index_tip.y - thumb_tip.y) ** 2 +\n",
    "                (index_tip.z - thumb_tip.z) ** 2) ** 0.5\n",
    "\n",
    "    # Adjust threshold based on typical 3D distances observed\n",
    "    return distance < 0.05\n",
    "\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Press 'Esc' to exit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a mirror-like effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get index finger tip coordinates\n",
    "            index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "            # Normalize coordinates to screen dimensions\n",
    "            cursor_x = int(index_finger_tip.x * screen_width)\n",
    "            cursor_y = int(index_finger_tip.y * screen_height)\n",
    "\n",
    "            # Move the mouse cursor\n",
    "            pyautogui.moveTo(cursor_x, cursor_y)\n",
    "\n",
    "            # Detect click gesture\n",
    "            if is_click_gesture(hand_landmarks):\n",
    "                pyautogui.click()\n",
    "                cv2.putText(frame, \"Click!\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Gesture-Based Cursor Control\", frame)\n",
    "\n",
    "    # Break the loop when 'Esc' key is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk, ImageDraw, ImageFont\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the main Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Gesture-Controlled Instagram Feed\")\n",
    "root.geometry(\"1024x768\")\n",
    "root.configure(bg=\"#f0f0f0\")\n",
    "\n",
    "# Load photos into a list\n",
    "photo_folder = \"photos/\"\n",
    "photo_files = [os.path.join(photo_folder, f) for f in os.listdir(photo_folder) if f.endswith((\".jpg\", \".png\"))]\n",
    "photos = [Image.open(photo).resize((400, 400)) for photo in photo_files]\n",
    "\n",
    "# Create a label to display a single photo\n",
    "photo_label = tk.Label(root, bg=\"#ffffff\", width=400, height=400)\n",
    "photo_label.pack(side=tk.LEFT, padx=10, pady=10)\n",
    "\n",
    "# Feedback label\n",
    "feedback_label = tk.Label(root, text=\"Perform gestures to interact!\", font=(\"Helvetica\", 14), bg=\"#f0f0f0\")\n",
    "feedback_label.pack(side=tk.BOTTOM, pady=20)\n",
    "\n",
    "# Interaction states\n",
    "current_photo_index = 0  # Start with the first photo\n",
    "liked_photos = {}\n",
    "disliked_photos = {}\n",
    "saved_photos = []  # List to store saved photos\n",
    "showing_saved_photos = False  # Track whether we are showing saved photos\n",
    "\n",
    "# Camera feed frame\n",
    "camera_frame = tk.Label(root, bg=\"#000000\", width=500, height=700)\n",
    "camera_frame.pack(side=tk.RIGHT, padx=10, pady=10)\n",
    "\n",
    "# Function to add emoji to photos\n",
    "def add_emoji(photo, emoji):\n",
    "    \"\"\"Add an emoji to the given photo.\"\"\"\n",
    "    img = photo.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 50)  # Ensure you have a compatible font installed\n",
    "    draw.text((150, 150), emoji, fill=\"red\", font=font)\n",
    "    return img\n",
    "\n",
    "# Function to update the displayed photo\n",
    "def update_photo():\n",
    "    \"\"\"Update the currently displayed photo based on the index.\"\"\"\n",
    "    global current_photo_index\n",
    "    if showing_saved_photos:\n",
    "        if len(saved_photos) == 0:\n",
    "            feedback_label.config(text=\"No saved photos available!\")\n",
    "            photo_label.config(image=\"\")\n",
    "            return\n",
    "        if 0 <= current_photo_index < len(saved_photos):\n",
    "            img = ImageTk.PhotoImage(saved_photos[current_photo_index])\n",
    "            photo_label.configure(image=img)\n",
    "            photo_label.img = img  # Keep a reference to avoid garbage collection\n",
    "    else:\n",
    "        if 0 <= current_photo_index < len(photos):\n",
    "            current_photo = photos[current_photo_index]\n",
    "            if current_photo_index in liked_photos:\n",
    "                current_photo = liked_photos[current_photo_index]\n",
    "            elif current_photo_index in disliked_photos:\n",
    "                current_photo = disliked_photos[current_photo_index]\n",
    "            img = ImageTk.PhotoImage(current_photo)\n",
    "            photo_label.configure(image=img)\n",
    "            photo_label.img = img  # Keep a reference to avoid garbage collection\n",
    "\n",
    "# Function to toggle between all photos and saved photos\n",
    "def show_saved_photos():\n",
    "    global showing_saved_photos, current_photo_index\n",
    "    showing_saved_photos = not showing_saved_photos\n",
    "    current_photo_index = 0  # Reset to the first photo\n",
    "    if showing_saved_photos:\n",
    "        feedback_label.config(text=\"Showing Saved Photos üìÇ\")\n",
    "    else:\n",
    "        feedback_label.config(text=\"Showing All Photos üåç\")\n",
    "    update_photo()\n",
    "\n",
    "# Gesture Detection Logic\n",
    "def detect_gesture(hand_landmarks):\n",
    "    \"\"\"Detect gestures for liking, disliking, saving, scrolling, and clicking.\"\"\"\n",
    "    global current_photo_index\n",
    "\n",
    "    # Get landmarks for gestures\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "    # Like Gesture (Thumbs Up)\n",
    "    if (\n",
    "        thumb_tip.y < index_tip.y\n",
    "        and thumb_tip.y < middle_tip.y\n",
    "        and thumb_tip.y < ring_tip.y\n",
    "        and thumb_tip.y < pinky_tip.y\n",
    "    ):\n",
    "        feedback_label.config(text=\"Photo Liked! ‚ù§Ô∏è\")\n",
    "        if current_photo_index not in liked_photos:\n",
    "            liked_photos[current_photo_index] = add_emoji(photos[current_photo_index], \"‚ù§Ô∏è\")\n",
    "        update_photo()\n",
    "        time.sleep(0.5)\n",
    "        return\n",
    "\n",
    "    # Dislike Gesture (Thumbs Down)\n",
    "    if (\n",
    "        thumb_tip.y > index_tip.y\n",
    "        and thumb_tip.y > middle_tip.y\n",
    "        and thumb_tip.y > ring_tip.y\n",
    "        and thumb_tip.y > pinky_tip.y\n",
    "    ):\n",
    "        feedback_label.config(text=\"Photo Disliked! üëé\")\n",
    "        if current_photo_index not in disliked_photos:\n",
    "            disliked_photos[current_photo_index] = add_emoji(photos[current_photo_index], \"üëé\")\n",
    "        update_photo()\n",
    "        time.sleep(0.5)\n",
    "        return\n",
    "\n",
    "    # Save Gesture (Peace Sign)\n",
    "    if (\n",
    "        index_tip.y < middle_tip.y  # Index finger is above middle finger\n",
    "        and middle_tip.y < ring_tip.y  # Middle finger is above ring finger\n",
    "        and ring_tip.y < pinky_tip.y  # Ring finger is above pinky\n",
    "        and thumb_tip.y > index_tip.y  # Thumb is below the index\n",
    "    ):\n",
    "        feedback_label.config(text=\"Photo Saved! ‚úåÔ∏è\")\n",
    "        current_photo = photos[current_photo_index]\n",
    "        if current_photo not in saved_photos:\n",
    "            saved_photos.append(current_photo)\n",
    "        time.sleep(0.5)\n",
    "        return\n",
    "\n",
    "    # Scrolling Gesture (Index Finger Movement)\n",
    "    if index_tip.y < hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].y:\n",
    "        feedback_label.config(text=\"Scrolling Up ‚¨ÜÔ∏è\")\n",
    "        if current_photo_index > 0:\n",
    "            current_photo_index -= 1\n",
    "            update_photo()\n",
    "        time.sleep(0.5)\n",
    "    elif index_tip.y > hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].y:\n",
    "        feedback_label.config(text=\"Scrolling Down ‚¨áÔ∏è\")\n",
    "        if showing_saved_photos:\n",
    "            if current_photo_index < len(saved_photos) - 1:\n",
    "                current_photo_index += 1\n",
    "                update_photo()\n",
    "        else:\n",
    "            if current_photo_index < len(photos) - 1:\n",
    "                current_photo_index += 1\n",
    "                update_photo()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Open webcam and process gestures\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def update_camera_feed():\n",
    "    \"\"\"Update the camera feed and process gestures in real-time.\"\"\"\n",
    "    global current_photo_index\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return\n",
    "\n",
    "    # Flip the frame horizontally for a mirror-like effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the frame to RGB for MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame for hand landmarks\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Detect gestures for liking, disliking, saving, scrolling, and clicking\n",
    "            detect_gesture(hand_landmarks)\n",
    "\n",
    "    # Convert the frame to an image for Tkinter\n",
    "    frame_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    imgtk = ImageTk.PhotoImage(image=frame_image)\n",
    "\n",
    "    # Display the camera feed in the GUI\n",
    "    camera_frame.imgtk = imgtk\n",
    "    camera_frame.configure(image=imgtk)\n",
    "\n",
    "    # Schedule the next frame update\n",
    "    root.after(10, update_camera_feed)\n",
    "\n",
    "# Start with the first photo\n",
    "update_photo()\n",
    "\n",
    "# Add a button to toggle saved photos\n",
    "saved_button = tk.Button(root, text=\"Show Saved Photos\", command=show_saved_photos, font=(\"Helvetica\", 12))\n",
    "saved_button.pack(side=tk.BOTTOM, pady=10)\n",
    "\n",
    "# Start the camera feed\n",
    "update_camera_feed()\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "root.mainloop()\n",
    "\n",
    "# Release the camera\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-10T14:16:18.783642Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class GestureApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.geometry(\"800x600\")\n",
    "        self.root.title(\"Gesture Controlled App\")\n",
    "\n",
    "        # Placeholder image\n",
    "        self.image_index = 0\n",
    "        self.images = self.fetch_images()  # Fetch Instagram images\n",
    "        self.image_label = tk.Label(root)\n",
    "        self.image_label.pack()\n",
    "\n",
    "        # Buttons\n",
    "        self.like_button = tk.Button(root, text=\"Like\", command=self.like_picture)\n",
    "        self.like_button.pack(side=tk.LEFT)\n",
    "\n",
    "        self.dislike_button = tk.Button(root, text=\"Dislike\", command=self.dislike_picture)\n",
    "        self.dislike_button.pack(side=tk.LEFT)\n",
    "\n",
    "        # Camera feed\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.update_camera_feed()\n",
    "\n",
    "    def fetch_images(self): #FILL IN FUNCTION\n",
    "        images = []\n",
    "        return images\n",
    "\n",
    "    def update_camera_feed(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            print(\"Camera not working!\")\n",
    "            return\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Process frame for hand landmarks (not shown here)\n",
    "\n",
    "        # Convert the frame to an image format that Tkinter can use\n",
    "        img = Image.fromarray(rgb_frame)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        self.image_label.imgtk = imgtk\n",
    "        self.image_label.configure(image=imgtk)\n",
    "\n",
    "        # Call this method again after 10 milliseconds\n",
    "        self.root.after(10, self.update_camera_feed)\n",
    "\n",
    "    def like_picture(self):\n",
    "        print(\"Liked picture\")\n",
    "\n",
    "    def dislike_picture(self):\n",
    "        print(\"Disliked picture\")\n",
    "\n",
    "# Create the Tkinter window and run the app\n",
    "root = tk.Tk()\n",
    "app = GestureApp(root)\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RAjfOu8vbX9T",
    "5WqLvhGEczPc",
    "AnVjzQPedaun"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
